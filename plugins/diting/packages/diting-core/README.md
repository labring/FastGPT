# ğŸ§  diting-core: Core Evaluation Engine for LLM Applications  

`diting-core` is the foundational module of the DiTing framework, responsible for the core logic of LLM application evaluation. It provides essential components for defining evaluation cases, computing metrics, interacting with models, and managing evaluation workflowsâ€”serving as the "brain" that powers DiTingâ€™s assessment capabilities.  


## ğŸ¯ Core Purpose  

`diting-core` focuses on enabling robust, customizable evaluation of LLM applications by abstracting key evaluation workflows. Its primary goals are:  
1. Standardize the structure of evaluation cases across tasks (QA, generation, classification, etc.).  
2. Provide out-of-the-box metrics for measuring LLM performance (accuracy, relevance, safety, etc.).  
3. Simplify integration with LLMs (via model clients) and evaluation pipelines (via callbacks).  
4. Support extension for custom tasks, metrics, and synthesis logic.  


## ğŸ” Key Components  

### 1. `callbacks` â€“ Evaluation Workflow Management  
Handles logging, progress tracking, and error handling during evaluations.  

- **Core Features**:  
  - `CallbackManager`: Orchestrates callbacks (e.g., logging, metrics aggregation) during evaluation runs.  
  - Built-in handlers for:  
    - Console logging (real-time progress updates).  
    - Metric aggregation (collecting results across batches).  
    - Error tracking (capturing failures in LLM calls or metric computation).  

- **Use Case**: Track evaluation latency, log failed cases, or send results to a monitoring system (e.g., Prometheus) in real time.  


### 2. `utilities` â€“ Shared Helper Tools  
A collection of utility functions and classes used across the module:  

- task processing (dispatch coroutine etc.).  
- Type validation (ensuring consistency in evaluation data).  
- Common constants (task types, metric names).  
- Helper functions for async operations (critical for LLM API calls).  


### 3. `cases` â€“ Evaluation Case Definitions  
Defines structured schemas for evaluation cases (`LLMCase`) to ensure consistency across tasks.  

- **Core Schema**:  
  - `user_input`: The prompt/query sent to the LLM (e.g., a question for QA tasks).  
  - `expected_output`: The expected/ground-truth output (e.g., the correct answer).  
  - `actual_output`: The actual output generated by the LLM (populated during evaluation).  
  - `context`: A list of contextual information related to the test case.  
  - `retrieval_context`: A list of retrieved context documents or information.  
  - `metadata`: Optional context (e.g., difficulty level, task category).  

- **Task-Specific Extensions**:  
  - `QACase`: Extends `LLMCase` with fields like `context` (background knowledge for QA).  
  - `ClassificationCase`: Adds `labels` (allowed categories) for classification tasks.  


### 4. `metrics` â€“ Evaluation Metrics  
Implements a library of metrics to assess LLM performance, with support for custom extensions via the `BaseMetric` abstract class.  

#### **Core Concepts**  
- **`MetricValue`**: The standard output format for all metrics, containing:  
  - `score`: A numerical value (e.g., 0.8 for 80% accuracy).  
  - `reason`: A human-readable explanation (e.g., "Output matches ideal answer").  
  - `run_logs`: Metadata about the computation (e.g., LLM API calls, latency).  

- **`BaseMetric`**: The abstract base class for all metrics, enforcing a consistent workflow:  
  1. **Input Validation**: Checks if the `LLMCase` contains required fields (via `_required_params`).  
  2. **Callback Integration**: Tracks progress and logs via DiTingâ€™s callback system (e.g., for debugging or monitoring).  
  3. **Async Computation**: Uses `_compute` (abstract method) to implement metric logic asynchronously.  


#### **Constructed Metrics**  
- **Answer similarity**:

  The **Answer Similarity** metric measures how similar the `actual_output` is to the `expected_output` using **embedding-based cosine similarity**.  
  It is especially useful for evaluating open-ended generation tasks where answers may differ lexically but are semantically equivalent.

  - **Range**: Typically [0, 1]
  - **Higher is better**: A score closer to 1 indicates stronger semantic similarity.

  **Definition**  
  This metric uses an embedding model (e.g., Sentence-BERT, BGE, OpenAI embeddings) to convert both outputs into vector representations, then computes the cosine similarity between the two vectors.

  **Steps to Compute**

  1. **Compute Embeddings**:  
     Convert both `actual_output` and `expected_output` into embedding vectors using the embedding model.

  2. **Normalize Vectors**:  
     Normalize both vectors to unit length.

  3. **Score Calculation**:
     $$
     Answer\ Similarity = \cos(\theta) = \frac{ \vec{v}_1 \cdot \vec{v}_2 }{ \| \vec{v}_1 \| \cdot \| \vec{v}_2 \| }
     $$

  Where:
  $$
  \vec{v}_1 = \text{embedding of actual output}, \quad
  \vec{v}_2 = \text{embedding of expected output}
  $$
  A higher score indicates the outputs are semantically more aligned.

- **Answer Correctness**: 

  The **Answer Correctness** metric evaluates the factual accuracy and semantic similarity of an actual response (`actual_output`) compared to an expected response (`expected_output`), based on a given user input (`user_input`). It leverages a Large Language Model (LLM) as a judge to assess the correctness of the answer.

    - **Range**: Typically [0, 1]  
    - **Higher is better**: A score closer to 1 indicates that the actual output is more accurate and semantically similar to the expected output.

  **Definition**  
    An answer is deemed correct if it aligns factually and semantically with the expected response, as evaluated by an LLM judge.

  **Steps to Compute**

    1. **Generate Statements**:  
       Utilize the LLM to extract factual or inferable statements from both the `actual_output` and `expected_output` in response to the `user_input`.

    2. **Generate Verdicts**:  
       Assess the generated statements to determine their factual accuracy and semantic similarity, producing verdicts on the correctness of each claim.

    3. **Compute Statement Presence**:  
       Calculate the presence of accurate statements using metrics such as F1 score, considering true positives (TP), false positives (FP), and false negatives (FN).

       **F1 Score Definition**:
       
       - **Precision** measures the proportion of correctly predicted positive instances among all predicted positives:
          $$
              Precision = \frac{TP}{TP + FP}
          $$
       
       - **Recall** measures the proportion of correctly predicted positive instances among all actual positives:
          $$
              Recall = \frac{TP}{TP + FN}
          $$
       
       - **F1 score** balances precision and recall, calculated as:
          $$
          F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
          $$

  4. **Compute Similarity Score**:  
       If semantic similarity is included, use an auxiliary metric to measure the similarity between the `actual_output` and `expected_output`.

    5. **Final Score Calculation**:  
       $$
       Answer\ Correctness\ Score = \text{Weighted Average}(F1\ Score, Similarity\ Score)
       $$

  The final score incorporates weights defined for factual accuracy and semantic similarity, allowing a balanced evaluation based on user preferences.

- **Answer Relevance**: 

  

- **Faithfulness**:

  The **Faithfulness** metric measures how factually consistent a response is with the retrieved context.

  It is especially useful in evaluating RAG (Retrieval-Augmented Generation) systems.

  - **Range**: Typically [0, 1]
  - **Higher is better**: A score closer to 1 indicates that the response is more consistent with the context.

  **Definition**
  A response is considered faithful if all of its claims can be supported by the retrieved context.

  **Steps to Compute**

  1. **Extract Claims**:
      Identify all factual or inferable statements in the `actual_output`.

  2. **Generate Verdicts**:
      For each claim, determine if it can be directly inferred or supported by the `retrieved_context`.

  3. **Score Calculation**:
     $$
     Faithfulness\ Score = \frac{\text{Number of supported claims}}{\text{Total number of claims}}
     $$

- **Context Recall**: 

  The **Context Recall** metric measures how many of the relevant documents or pieces of information were successfully retrieved for a given query. It emphasizes not missing important information, ensuring that the retrieval step captures as much of the relevant context as possible.

  - **Range**:  Typically [0, 1]
  - **Higher is better**: A higher recall score indicates fewer relevant documents or information pieces were omitted.

  **Definition**  

  Recall focuses on completeness: the proportion of relevant information successfully retrieved compared to the total relevant information available. Calculating context recall requires a reference to compare against, typically a set of ground-truth relevant contexts.

  **Steps to Compute**

  1. **Extract Claims**:

     The `reference` answer is broken down into individual claims.

  2. **Generate Verdicts**:

     Each claim is analyzed to determine whether it can be attributed (i.e., supported) by the `retrieved_contexts`.

  3. **Score Calculation**:

     Ideally, all claims in the `reference` answer should be supported by the retrieved contexts for perfect recall.

  $$
  Context\ Recall = \frac{\text{Number of claims supported by retrieved context}}{\text{Total number of claims in reference}}
  $$

- **Context Precision**: 

  **Context Precision** is a metric that measures the proportion of relevant chunks in the `retrieved_contexts`. It evaluates how much of the retrieved information is truly useful or relevant to the task at hand.

  - **Range**: Typically [0, 1]
  - **Higher is better**: A higher score indicates that more of the retrieved chunks are relevant and that the retrieved context is of higher quality.

  **Definition**  
  Context Precision is computed as the average of Precision@k over all retrieved chunks. Precision@k measures how many of the top-k retrieved chunks are relevant.

  ---

  **Steps to Compute**

  1. **Label Relevance**  
     Each chunk in the `retrieved_contexts` is labeled as relevant (1) or not relevant (0) based on whether it supports the reference answer or not.

  2. **Compute Precision@k**  
     For each rank \( k \), calculate:

     $$
     Precision@k = \frac{\text{Number of relevant chunks in top } k}{k}
     $$

  3. **Calculate Final Precision Score**  
     The final **Context Precision** score is the average over all \( k \) positions:

     $$
     Context\ Precision = \frac{1}{N} \sum_{k=1}^{N} Precision@k
     $$

     where \( N \) is the total number of retrieved chunks.

  ---

  **Notes**  
  - High precision means the retrieved results are mostly relevant and reduce noise.
  - This metric complements **Context Recall**, where recall focuses on coverage (not missing relevant info), while precision emphasizes correctness (not retrieving irrelevant info).

- **QA Quality**: .

- **RAG Runtime**: .


#### **Custom Metric Example**  
To define a task-specific metric, inherit from `BaseMetric` and implement `_compute`:  
```python
from diting_core.metrics.base_metric import BaseMetric, MetricValue, LLMCaseParams
from diting_core.cases.llm_case import LLMCase

class KeywordPresenceMetric(BaseMetric):
    # Require the test case to have an "output" field
    _required_params = [LLMCaseParams.ACTUAL_OUTPUT]

    async def _compute(self, test_case: LLMCase, **kwargs) -> MetricValue:
        # Check if "critical_keyword" is in the LLM's output
        has_keyword = "critical_keyword" in test_case.output.lower()
        score = 1.0 if has_keyword else 0.0
        reason = "Keyword found" if has_keyword else "Keyword missing"
        
        return MetricValue(score=score, reason=reason)
```

**Usage**:  
```python
metric = KeywordPresenceMetric()
test_case = LLMCase(actual_output="This contains critical_keyword")
result = await metric.compute(test_case)
print(result.score)  # 1.0
print(result.reason)  # "Keyword found"
```


#### **Key Features of `BaseMetric`**  
-** Input Validation **: Ensures `LLMCase` has required fields (via `_required_params`).  
-** Callback Support **: Integrates with DiTingâ€™s callback system for logging/monitoring.  
-** Async Compatibility **: Enables efficient batch processing (critical for large datasets).  
-** Explainability **: Captures `reason` to justify scores (useful for debugging).

### 5. `models` â€“ LLM Client Integration  
Provides a unified interface to interact with LLMs (both open-source and proprietary APIs).  

- **Supported Clients**:  
  - OpenAI API (GPT-3.5/4).  
  - Hugging Face `transformers` (local models like LLaMA, Mistral).  
  - Custom endpoints (via `BaseLLM` abstract class).  

- **Key Features**:  
  - Async support for efficient batch processing.  
  - Token counting (via `tiktoken` or model-specific logic).  
  - Retry logic for handling API timeouts/failures.  


### 6. `synthesis` â€“ Evaluation Data Generation  
Generates synthetic evaluation cases from raw corpora (e.g., creating QA pairs from textbooks).  

- **Core Components**:  
  - `BaseSynthesizer`: Abstract class for defining synthesis logic (e.g., QA, summarization).  
  - `QASynthesizer`: Generates `QACase` instances by prompting LLMs to create questions/answers from context.  
  - `BaseCorpusGenerator`: Abstract class for defining corpora generation logic(e.g., generate corpora from documents).  

#### **BaseSynthesizer** Interface  
The foundational class for all synthesizers, providing:  
- **Input/Output Validation**: Ensures corpora contain required fields (`required_input_fields`) and generated cases meet schema expectations (`required_output_fields`).  
- **Callback Integration**: Tracks synthesis progress via DiTing's callback system.  
- **Error Handling**: Validates data integrity and logs exceptions.  

#### Pre-built Synthesizers:  
- **QA Synthesizer**: Generates question-answer pairs from factual text.  

Example implementation:

```python
from diting_core.synthesis import BaseSynthesizer
from diting_core.cases.llm_case import LLMCase
from diting_dataset.corpus import GraphBasedCorpus


class CustomSynthesizer(BaseSynthesizer):
  required_input_fields = ["context"]  # Corpus must have 'context' field
  required_output_fields = ["question", "answer"]  # Output must have these fields

  async def _apply(self, corpus: GraphBasedCorpus, **kwargs) -> LLMCase:
    # Use LLM to generate question-answer pair from corpus context
    question = await self.model.generate(f"Create a question from: {corpus.context}")
    answer = await self.model.generate(f"Answer: {question} based on: {corpus.context}")

    return LLMCase(
      input={"question": question},
      ideal={"answer": answer},
      metadata={"corpus_id": corpus.id}
    )
```

#### **BaseCorpusGenerator** Interface  
`BaseCorpusGenerator` is an abstract base class for generating `BaseCorpus` instances. Its core workflow includes:  
1. Building a knowledge graph from raw data.  
2. Aggregating graph nodes to form clusters.  
3. Generating subgraphs from clusters.  
4. Constructing `BaseCorpus` objects from subgraphs.  

Example implementation:

```python
from diting_core.synthesis.base_corpus import BaseCorpusGenerator, BaseCorpus

class CustomCorpusGenerator(BaseCorpusGenerator):
  async def _generate_corpora(self, num_corpora: int, **kwargs) -> List[BaseCorpus]:
    # Implement custom logic to generate corpora
    return [
      BaseCorpus(
        context=["Sample context text"],
        scenario="custom_scenario",
      )
      for _ in range(num_corpora)
    ]


# Generate 10 corpora with perfect grammar and medium length
generator = CustomCorpusGenerator()
corpora = await generator.generate_corpora(num_corpora=10)
```

## ğŸš€ Quickstart  

### 1. Install `diting-core`  
```bash
# From the parent project root
uv sync --group core  # Assumes a dependency group for core
```

### 2. Basic Evaluation Workflow  
```python
from diting_core.cases.llm_case import LLMCase
from diting_core.metrics.answer_correctness.answer_correctness import AnswerCorrectness
from diting_core.models import OpenAIClient
from diting_core.utilities.executor import as_completed

# 1. Define evaluation cases
cases = [
    LLMCase(input="2+2", ideal="4"),
    LLMCase(input="Capital of France", ideal="Paris")
]

# 2. Initialize LLM client
llm = OpenAIClient(model="gpt-3.5-turbo")

# 3. Run LLM on cases
for case in cases:
    case.output = llm.generate(case.input)  # Populate LLM output

# 4. Evaluate with accuracy metric
metric = AnswerCorrectness()
results = [await metric.compute(case) for case in cases]
print(f"Accuracy: {sum(results)/len(results):.2f}")  # e.g., 1.0 (100% correct)
```


## ğŸ› ï¸ Development Guide  

### Environment Requirements  
- Python 3.11+  
- Dependencies managed via `uv` (see `pyproject.toml` for details).  

### Project Structure  
```
diting-core/
â”œâ”€â”€ callbacks/          # Workflow management (logging, progress)
â”œâ”€â”€ utilities/          # Shared helper tools
â”œâ”€â”€ cases/              # Evaluation case schemas (LLMCase, QACase)
â”œâ”€â”€ metrics/            # Evaluation metrics (Accuracy, BLEU, etc.)
â”œâ”€â”€ models/             # LLM clients (OpenAI, Hugging Face)
â””â”€â”€ synthesis/          # Synthetic data generation (QA, summarization)
```