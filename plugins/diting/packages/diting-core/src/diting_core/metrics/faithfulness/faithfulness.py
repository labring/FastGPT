#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from dataclasses import field, dataclass
from typing import Type, Any, List, Optional, cast

from diting_core.callbacks.base import Callbacks
from diting_core.callbacks.manager import new_group
from diting_core.cases.llm_case import LLMCase, LLMCaseParams
from diting_core.metrics.base_metric import BaseMetric, MetricValue
from diting_core.models.llms.base_model import BaseLLM
from diting_core.metrics.faithfulness.template import FaithfulnessTemplate
from diting_core.metrics.faithfulness.schema import (
    Statements,
    Verdicts,
    FaithfulnessVerdict,
    Reason,
)


@dataclass
class Faithfulness(BaseMetric):
    """
    The faithfulness metric uses an LLM-as-a-judge approach to evaluate
    how factually consistent the actual_output is with respect to the provided retrieval context.

    This metric is particularly useful for assessing the reliability and factual grounding
    of a RAG pipeline's generation by measuring whether the content in the output can be
    faithfully inferred from the retrieved context.

    Constraints:
        To use the Faithfulness metric, you'll have to provide the following arguments when creating an LLMTestCase:
        - user_input: The original query or prompt provided by the user.
        - actual_output: The output generated by your RAG system.
        - retrieval_context: The documents retrieved to help the model generate the actual_output.
        These inputs are required for all LLMTestCase instances and used directly in the faithfulness evaluation.

    Attributes:
        model (Optional[BaseLLM]): The judge model used to assess whether the generated statements are faithful to the retrieval context.
        evaluation_template (FaithfulnessTemplate): The prompt template used to guide the LLM during statement generation and verdict evaluation.
    """

    model: Optional[BaseLLM] = None
    _required_params: List[LLMCaseParams] = field(
        default_factory=lambda: [
            LLMCaseParams.USER_INPUT,
            LLMCaseParams.ACTUAL_OUTPUT,
            LLMCaseParams.RETRIEVAL_CONTEXT,
        ]
    )
    include_reason: bool = True
    evaluation_template: Type[FaithfulnessTemplate] = FaithfulnessTemplate

    @staticmethod
    def _compute_score(verdicts: Verdicts) -> float:
        # check the verdicts and compute the score
        try:
            import numpy as np
        except ImportError:
            raise ImportError(
                "This function requires 'numpy'. Install it with: pip install numpy"
            )
        faithful_statements = sum(
            1 if verdict.verdict else 0 for verdict in verdicts.verdicts
        )
        num_statements = len(verdicts.verdicts)
        if num_statements:
            score = faithful_statements / num_statements
        else:
            score = np.nan

        return score

    async def _a_generate_statements(
        self, user_input: str, text: str, callbacks: Optional[Callbacks] = None
    ) -> List[str]:
        assert self.model is not None, "llm is not set"
        prompt = self.evaluation_template.generate_statements(
            user_input=user_input, text=text
        )
        run_mgt, grp_cb = await new_group(
            name="generate_statements",
            inputs={"user_input": user_input, "text": text},
            callbacks=callbacks,
        )
        try:
            res = cast(
                Statements,
                await self.model.generate_structured_output(
                    prompt, schema=Statements, callbacks=grp_cb
                ),
            )
            statements: List[str] = res.statements
        except Exception as e:
            await run_mgt.on_chain_error(e)
            raise e

        await run_mgt.on_chain_end(outputs={"statements": statements})
        return statements

    async def _a_generate_verdicts(
        self,
        retrieval_context: List[str],
        statements: List[str],
        callbacks: Optional[Callbacks] = None,
    ) -> Verdicts:
        assert self.model is not None, "llm is not set"
        contexts_str: str = "\n".join(retrieval_context)
        prompt = self.evaluation_template.generate_verdicts(
            retrieval_context=contexts_str, statements=statements
        )
        run_mgt, grp_cb = await new_group(
            name="generate_verdicts",
            inputs={"retrieval_context": contexts_str, "statements": statements},
            callbacks=callbacks,
        )
        try:
            verdicts = await self.model.generate_structured_output(
                prompt, schema=Verdicts, callbacks=grp_cb
            )
        except Exception as e:
            await run_mgt.on_chain_error(e)
            raise e
        await run_mgt.on_chain_end(outputs={"verdicts": verdicts})
        return cast(Verdicts, verdicts)

    async def _a_generate_reason(
        self,
        score: float,
        verdicts: List[FaithfulnessVerdict],
        callbacks: Optional[Callbacks] = None,
    ) -> str:
        assert self.model is not None, "llm is not set"
        contradictions: List[str] = []
        for verdict in verdicts:
            if verdict.verdict == 0:
                contradictions.append(verdict.reason)
        prompt = self.evaluation_template.generate_reason(
            score=round(score, 2), contradictions=contradictions
        )
        run_mgt, grp_cb = await new_group(
            name="generate_reason",
            inputs={
                "score": score,
                "verdicts": verdicts,
            },
            callbacks=callbacks,
        )
        try:
            res = cast(
                Reason,
                await self.model.generate_structured_output(
                    prompt, schema=Reason, callbacks=grp_cb
                ),
            )
        except Exception as e:
            await run_mgt.on_chain_error(e)
            raise e
        await run_mgt.on_chain_end(outputs={"reason": res.reason})
        return res.reason

    async def _compute(
        self,
        test_case: LLMCase,
        *args: Any,
        callbacks: Optional[Callbacks] = None,
        **kwargs: Any,
    ) -> MetricValue:
        assert test_case.user_input, "user_input cannot be empty"
        assert test_case.actual_output, "actual_output cannot be empty"
        assert test_case.retrieval_context, "retrieval_context cannot be empty"
        statements = await self._a_generate_statements(
            user_input=test_case.user_input,
            text=test_case.actual_output,
            callbacks=callbacks,
        )
        try:
            import numpy as np
        except ImportError:
            raise ImportError(
                "This function requires 'numpy'. Install it with: pip install numpy"
            )
        if not statements:
            metric_value = MetricValue(
                score=np.nan,
                run_logs={
                    "statements": statements,
                },
            )
            return metric_value
        verdicts = await self._a_generate_verdicts(
            retrieval_context=test_case.retrieval_context,
            statements=statements,
            callbacks=callbacks,
        )
        score = self._compute_score(verdicts)
        reason = None
        if self.include_reason:
            reason = await self._a_generate_reason(
                score, verdicts.verdicts, callbacks=callbacks
            )
        metric_value = MetricValue(
            score=score,
            reason=reason,
            run_logs={"statements": statements, "verdicts": verdicts},
        )
        return metric_value

