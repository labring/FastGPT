---
title: Try Open-Source Models with SiliconCloud
description: Experience open-source models through SiliconCloud
---

[SiliconCloud](https://cloud.siliconflow.cn/i/TR9Ym0c4) is a platform focused on providing open-source model APIs with its own acceleration engine. It helps users test and use open-source models quickly and cost-effectively. In practice, their models offer excellent speed and stability with rich variety, covering language, embedding, rerank, TTS, STT, image generation, and video generation models — meeting all of FastGPT's model needs.

If you want to use SiliconCloud for only some models, see [OneAPI Integration with SiliconCloud](/docs/introduction/development/modelconfig/one-api/#siliconcloud--open-source-model-collection).

This guide covers deploying FastGPT entirely with SiliconCloud models.

## 1. Register for a SiliconCloud Account

1. [Register for a SiliconCloud account](https://cloud.siliconflow.cn/i/TR9Ym0c4)
2. Go to the console and get your API key: https://cloud.siliconflow.cn/account/ak

## 2. Update FastGPT Environment Variables

```bash
OPENAI_BASE_URL=https://api.siliconflow.cn/v1
# Enter the API Key from the SiliconCloud console
CHAT_API_KEY=sk-xxxxxx
```

## 3. Update FastGPT Model Configuration

The system includes several built-in SiliconCloud models for testing. You can manually add others if needed.

Here we've enabled `Qwen2.5 72b` for both text-only and vision models; selected `bge-m3` as the embedding model; selected `bge-reranker-v2-m3` as the rerank model; selected `fish-speech-1.5` for text-to-speech; and selected `SenseVoiceSmall` for speech-to-text.

![alt text](/imgs/image-104.png)

## 4. Testing

### Test Conversations and Image Recognition

Create a simple app, select the corresponding model, enable image upload, and test:

|                                 |                                 |
| ------------------------------- | ------------------------------- |
| ![alt text](/imgs/image-68.png) | ![alt text](/imgs/image-70.png) |

As you can see, the 72B model performs incredibly fast. Without a few 4090s locally, forget about the setup — just the output would take 30 seconds.

### Test Knowledge Base Import and Q&A

Create a new knowledge base (since only one embedding model is configured, the page won't show a model selector).

|                                 |                                 |
| ------------------------------- | ------------------------------- |
| ![alt text](/imgs/image-72.png) | ![alt text](/imgs/image-71.png) |

Import a local file by selecting it and clicking through the steps. 79 chunks indexed in about 20 seconds. Now let's test knowledge base Q&A.

Go back to the app we just created, select the knowledge base, adjust parameters, and start chatting:

|                                 |                                 |                                 |
| ------------------------------- | ------------------------------- | ------------------------------- |
| ![alt text](/imgs/image-73.png) | ![alt text](/imgs/image-75.png) | ![alt text](/imgs/image-76.png) |

After the conversation, click the citation at the bottom to view details, including specific retrieval and rerank scores:

|                                 |                                 |
| ------------------------------- | ------------------------------- |
| ![alt text](/imgs/image-77.png) | ![alt text](/imgs/image-78.png) |

### Test Voice Playback

In the same app, find **Voice Playback** in the left sidebar. Click it to select a voice model from the popup and preview it:

![alt text](/imgs/image-79.png)

### Test Voice Input

In the same app, find **Voice Input** in the left sidebar and enable it in the popup:

![alt text](/imgs/image-80.png)

Once enabled, a microphone icon appears in the chat input box. Click it to use voice input:

|                                 |                                 |
| ------------------------------- | ------------------------------- |
| ![alt text](/imgs/image-81.png) | ![alt text](/imgs/image-82.png) |

## Summary

If you want to quickly experience open-source models or get FastGPT up and running without applying for API keys from multiple providers, SiliconCloud is a great choice for rapid testing.

If you plan to self-host models and FastGPT in the future, you can use SiliconCloud for testing and validation upfront, then purchase hardware later — reducing POC time and cost.
