---
title: 'Integrating Local Models with Ollama'
description: 'Deploy your own models using Ollama'
---

[Ollama](https://ollama.com/) is an open-source AI model deployment tool that simplifies deploying and using large language models. It supports one-click download and running of various LLMs.

## Installing Ollama

Ollama supports multiple installation methods, but we recommend using Docker to pull and deploy the image. Installing Ollama directly on your personal device requires solving how to let the FastGPT container in Docker access Ollama on the host machine, which can be tricky.

### Docker Installation (Recommended)

You can use Ollama's official Docker image for one-click installation and startup (make sure Docker is installed on your machine):

```bash
docker pull ollama/ollama
docker run --rm -d --name ollama -p 11434:11434 ollama/ollama
```

If your FastGPT is deployed in Docker, make sure the Ollama image is on the same network as FastGPT when pulling it. Otherwise, FastGPT may not be able to access it:

```bash
docker run --rm -d --name ollama --network (your FastGPT container network) -p 11434:11434 ollama/ollama
```

### Host Installation

If you don't want to use Docker, you can install on the host machine. Here are some installation methods:

#### MacOS

If you're using macOS with Homebrew installed, install Ollama with:

```bash
brew install ollama
ollama serve # Start the service after installation
```

#### Linux

On Linux, you can use a package manager to install Ollama. For Ubuntu, run:

```bash
curl https://ollama.com/install.sh | sh # Downloads and runs the install script from the official site
ollama serve # Start the service after installation
```

#### Windows

On Windows, download the Windows installer from the Ollama official website. Run the installer and follow the wizard. After installation, start the service in Command Prompt or PowerShell:

```bash
ollama serve # After installation and starting the service, visit http://localhost:11434 in your browser to verify Ollama is installed successfully
```

#### Additional Notes

If you're using Ollama as a host application (not an image), make sure Ollama can listen on 0.0.0.0.

##### 1. Linux Systems

If Ollama runs as a systemd service, open a terminal and edit the Ollama systemd service file with `sudo systemctl edit ollama.service`. Add `Environment="OLLAMA_HOST=0.0.0.0"` in the [Service] section. Save and exit, then run `sudo systemctl daemon-reload` and `sudo systemctl restart ollama` to apply the configuration.

##### 2. MacOS Systems

Open a terminal and use `launchctl setenv ollama_host "0.0.0.0"` to set the environment variable, then restart the Ollama application for changes to take effect.

##### 3. Windows Systems

Open "Edit system environment variables" from the Start menu or search bar. In the "System Properties" window, click "Environment Variables". In the "System variables" section, click "New" and create a variable named OLLAMA_HOST with value 0.0.0.0. Click "OK" to save, then restart the Ollama application from the Start menu.

### Pull Ollama Model Images

After installing Ollama, there are no model images locally — you need to pull them yourself:

```bash
# For Docker deployment, enter the container first: docker exec -it [Ollama container name] /bin/sh
ollama pull [model name]
```

![](/imgs/Ollama-pull.png)

### Test Communication

After installation, test by entering the FastGPT container and trying to access your Ollama:

```bash
docker exec -it [FastGPT container name] /bin/sh
curl http://XXX.XXX.XXX.XXX:11434  # Container deployment address: "http://[container name]:[port]", host installation address: "http://[host IP]:[port]", host IP cannot be localhost
```

If you see your Ollama service running, communication is working.

## Integrating Ollama with FastGPT

### 1. Check Ollama's Available Models

First, check which models Ollama has:

```bash
# For Docker-deployed Ollama, use: docker exec -it [Ollama container name] /bin/sh
ollama ls
```

![](/imgs/Ollama-models1.png)

### 2. AI Proxy Integration

If you're using FastGPT's default config file deployment [here](/docs/introduction/development/docker.md), it uses AI Proxy by default.

![](/imgs/Ollama-aiproxy1.png)

Make sure your FastGPT can directly access the Ollama container. If not, refer to the [installation section](#installing-ollama) above to check if the host can't listen on 0.0.0.0 or if containers aren't on the same network.

![](/imgs/Ollama-aiproxy2.png)

In FastGPT, click Account -> Model Providers -> Model Configuration -> Add Model to add your model. Make sure the model ID matches the model name in OneAPI. See details [here](/docs/introduction/development/modelConfig/intro.md).

![](/imgs/Ollama-models2.png)

![](/imgs/Ollama-models3.png)

Run FastGPT, select Account -> Model Providers -> Model Channels -> Add Channel. In channel selection, choose Ollama, then add your pulled model and fill in the proxy address. For container-installed Ollama, the proxy address is http://address:port. Note: Container deployment address is "http://[container name]:[port]", host installation address is "http://[host IP]:[port]", host IP cannot be localhost.

![](/imgs/Ollama-aiproxy3.png)

In the workspace, create an app and select the model you added earlier. The model name here is the alias you set. Note: The same model cannot be added multiple times — the system will use the alias from the most recent addition.

![](/imgs/Ollama-models4.png)

### 3. OneAPI Integration

If you want to use OneAPI, first pull the OneAPI image and run it on FastGPT's container network:

```bash
# Pull oneAPI image
docker pull intel/oneapi-hpckit

# Run container with custom network and container name
docker run -it --network [FastGPT network] --name container_name intel/oneapi-hpckit /bin/bash
```

Go to the OneAPI page, add a new channel, select Ollama as the type, fill in your Ollama model (make sure the added model name matches the one in Ollama), then fill in your Ollama proxy address below, default http://address:port, no need to add /v1. After adding successfully, test the channel in OneAPI. If the test succeeds, the addition is successful. This demo uses Docker-deployed Ollama. For host Ollama, change the proxy address to http://[host IP]:[port].

![](/imgs/Ollama-oneapi1.png)

After adding the channel successfully, click Token, click Add Token, fill in the name, and modify the configuration.

![](/imgs/Ollama-oneapi2.png)

Modify the docker-compose.yml file for FastGPT deployment. Comment out AI Proxy usage, add your OneAPI open address in OPENAI_BASE_URL, default is http://address:port/v1, v1 must be included. Fill in your OneAPI token in KEY.

![](/imgs/Ollama-oneapi3.png)

[Jump directly to section 5](#5-model-addition-and-usage) to add and use models.

### 4. Direct Integration

If you don't want to use AI Proxy or OneAPI, you can choose direct integration. Modify the docker-compose.yml file for FastGPT deployment. Comment out AI Proxy usage, use similar configuration as OneAPI. Comment out AIProxy-related code, add your Ollama open address in OPENAI_BASE_URL, default is http://address:port/v1, emphasis: v1 must be included. Fill in KEY randomly, as Ollama has no authentication by default. If authentication is enabled, fill in accordingly. Other operations are the same as adding Ollama in OneAPI — just add your model in FastGPT to use it. This demo uses Docker-deployed Ollama. For host Ollama, change the proxy address to http://[host IP]:[port].

![](/imgs/Ollama-direct1.png)

After completion, [click here](#5-model-addition-and-usage) to add and use models.

### 5. Model Addition and Usage

In FastGPT, click Account -> Model Providers -> Model Configuration -> Add Model to add your model. Make sure the model ID matches the model name in OneAPI.

![](/imgs/Ollama-models2.png)

![](/imgs/Ollama-models3.png)

In the workspace, create an app and select the model you added earlier. The model name here is the alias you set. Note: The same model cannot be added multiple times — the system will use the alias from the most recent addition.

![](/imgs/Ollama-models4.png)

### 6. Additional Notes

In the Ollama proxy addresses above, the host installation Ollama address is "http://[host IP]:[port]", container-deployed Ollama address is "http://[container name]:[port]".
