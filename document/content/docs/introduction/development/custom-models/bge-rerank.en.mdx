---
title: Integrating bge-rerank Reranking Models
description: How to integrate bge-rerank reranking models
---

## Recommended Configuration by Model

Recommended specs:

| Model Name          | RAM    | VRAM   | Disk Space | Start Command    |
| ------------------- | ------ | ------ | ---------- | ---------------- |
| bge-reranker-base   | >=4GB  | >=4GB  | >=8GB      | python app.py    |
| bge-reranker-large  | >=8GB  | >=8GB  | >=8GB      | python app.py    |
| bge-reranker-v2-m3  | >=8GB  | >=8GB  | >=8GB      | python app.py    |

## Source Code Deployment

### 1. Install Environment

- Python 3.9 or 3.10
- CUDA 11.7
- VPN/proxy for accessing external resources

### 2. Download Code

Code for the 3 models:

1. [https://github.com/labring/FastGPT/tree/main/plugins/model/rerank-bge/bge-reranker-base](https://github.com/labring/FastGPT/tree/main/plugins/model/rerank-bge/bge-reranker-base)
2. [https://github.com/labring/FastGPT/tree/main/plugins/model/rerank-bge/bge-reranker-large](https://github.com/labring/FastGPT/tree/main/plugins/model/rerank-bge/bge-reranker-large)
3. [https://github.com/labring/FastGPT/tree/main/plugins/model/rerank-bge/bge-reranker-v2-m3](https://github.com/labring/FastGPT/tree/main/plugins/model/rerank-bge/bge-reranker-v2-m3)

### 3. Install Dependencies

```sh
pip install -r requirements.txt
```

### 4. Download Models

HuggingFace repositories for the 3 models:

1. [https://huggingface.co/BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
2. [https://huggingface.co/BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)
3. [https://huggingface.co/BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3)

Clone the model into the corresponding code directory. Directory structure:

```
bge-reranker-base/
app.py
Dockerfile
requirements.txt
```

### 5. Run the Code

```bash
python app.py
```

After successful startup, you should see an address like this:

![](/imgs/rerank1.png)

> The `http://0.0.0.0:6006` shown here is your connection address.

## Docker Deployment

**Image names:**

1. registry.cn-hangzhou.aliyuncs.com/fastgpt/bge-rerank-base:v0.1 (4 GB+)
2. registry.cn-hangzhou.aliyuncs.com/fastgpt/bge-rerank-large:v0.1 (5 GB+)
3. registry.cn-hangzhou.aliyuncs.com/fastgpt/bge-rerank-v2-m3:v0.1 (5 GB+)

**Port**

6006

**Environment Variables**

```
ACCESS_TOKEN=Your access token. Include in requests as: Authorization: Bearer ${ACCESS_TOKEN}
```

**Example Run Command**

```sh
# auth token is mytoken
docker run -d --name reranker -p 6006:6006 -e ACCESS_TOKEN=mytoken --gpus all registry.cn-hangzhou.aliyuncs.com/fastgpt/bge-rerank-base:v0.1
```

**docker-compose.yml Example**

```
version: "3"
services:
  reranker:
    image: registry.cn-hangzhou.aliyuncs.com/fastgpt/bge-rerank-base:v0.1
    container_name: reranker
    # GPU runtime environment. If not installed on host, comment out the deploy section
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    ports:
      - 6006:6006
    environment:
      - ACCESS_TOKEN=mytoken
```

## Integrate with FastGPT

1. Open FastGPT model configuration and add a new reranking model.
2. Fill in the model config form: Model ID is `bge-reranker-base`, URL is `{{host}}/v1/rerank`, where host is your deployed domain/IP:Port.

![alt text](/imgs/image-102.png)

## FAQ

### 403 Error

The custom request token in FastGPT doesn't match the ACCESS_TOKEN environment variable.

### Docker Shows `Bus error (core dumped)`

Try increasing the `shm_size` config in `docker-compose.yml` to expand the shared memory directory size in the container.

```
...
services:
  reranker:
    ...
    container_name: reranker
    shm_size: '2gb'
    ...
```
