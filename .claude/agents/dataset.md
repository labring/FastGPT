---
name: dataset-agent
description: çŸ¥è¯†åº“å¼€å‘ Agent,è´Ÿè´£ FastGPT çŸ¥è¯†åº“æ¨¡å—ä»£ç å¼€å‘ã€‚
model: inherit
color: blue
---

# FastGPT çŸ¥è¯†åº“(Dataset)æ¨¡å—æ¶æ„è¯´æ˜

## æ¦‚è¿°

FastGPT çŸ¥è¯†åº“æ¨¡å—æ˜¯ä¸€ä¸ªåŸºäº MongoDB + PostgreSQL(å‘é‡æ•°æ®åº“) çš„ RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)çŸ¥è¯†åº“ç³»ç»Ÿ,æ”¯æŒå¤šç§æ•°æ®æºå¯¼å…¥ã€æ™ºèƒ½æ–‡æ¡£åˆ†å—ã€å‘é‡åŒ–ç´¢å¼•ã€æ··åˆæ£€ç´¢ç­‰æ ¸å¿ƒèƒ½åŠ›ã€‚

## æ ¸å¿ƒæ¦‚å¿µå±‚æ¬¡ç»“æ„

```
Dataset (çŸ¥è¯†åº“)
  â”œâ”€â”€ DatasetCollection (æ–‡æ¡£é›†åˆ/æ–‡ä»¶)
  â”‚     â”œâ”€â”€ DatasetData (æ•°æ®å—/Chunk)
  â”‚     â”‚     â”œâ”€â”€ indexes[] (å‘é‡ç´¢å¼•)
  â”‚     â”‚     â””â”€â”€ history[] (å†å²ç‰ˆæœ¬)
  â”‚     â””â”€â”€ DatasetTraining (è®­ç»ƒé˜Ÿåˆ—)
  â””â”€â”€ Tag (æ ‡ç­¾ç³»ç»Ÿ)
```

### 1. Dataset (çŸ¥è¯†åº“)
- **ä½œç”¨**: æœ€é¡¶å±‚å®¹å™¨,å¯ä»¥æ˜¯æ™®é€šçŸ¥è¯†åº“ã€æ–‡ä»¶å¤¹ã€ç½‘ç«™çŸ¥è¯†åº“æˆ–å¤–éƒ¨æ•°æ®æº
- **ç±»å‹**:
  - `folder`: æ–‡ä»¶å¤¹ç»„ç»‡
  - `dataset`: æ™®é€šçŸ¥è¯†åº“
  - `websiteDataset`: ç½‘ç«™æ·±åº¦é“¾æ¥
  - `apiDataset`: API æ•°æ®é›†
  - `feishu`: é£ä¹¦çŸ¥è¯†åº“
  - `yuque`: è¯­é›€çŸ¥è¯†åº“
  - `externalFile`: å¤–éƒ¨æ–‡ä»¶

### 2. DatasetCollection (æ–‡æ¡£é›†åˆ)
- **ä½œç”¨**: çŸ¥è¯†åº“ä¸­çš„å…·ä½“æ–‡ä»¶æˆ–æ–‡æ¡£,æ‰¿è½½åŸå§‹æ•°æ®
- **ç±»å‹**:
  - `folder`: æ–‡ä»¶å¤¹
  - `file`: æœ¬åœ°æ–‡ä»¶
  - `link`: å•ä¸ªé“¾æ¥
  - `apiFile`: API æ–‡ä»¶
  - `images`: å›¾ç‰‡é›†åˆ
  - `virtual`: è™šæ‹Ÿé›†åˆ

### 3. DatasetData (æ•°æ®å—)
- **ä½œç”¨**: æ–‡æ¡£åˆ†å—åçš„æœ€å°çŸ¥è¯†å•å…ƒ,å®é™…æ£€ç´¢çš„å¯¹è±¡
- **æ ¸å¿ƒå­—æ®µ**:
  - `q`: é—®é¢˜æˆ–å¤§å—æ–‡æœ¬
  - `a`: ç­”æ¡ˆæˆ–è‡ªå®šä¹‰å†…å®¹
  - `indexes[]`: å‘é‡ç´¢å¼•åˆ—è¡¨(å¯å¤šä¸ª)
  - `chunkIndex`: å—ç´¢å¼•ä½ç½®
  - `imageId`: å…³è”å›¾ç‰‡ID
  - `history[]`: ä¿®æ”¹å†å²

### 4. DatasetTraining (è®­ç»ƒé˜Ÿåˆ—)
- **ä½œç”¨**: å¼‚æ­¥è®­ç»ƒä»»åŠ¡é˜Ÿåˆ—,è´Ÿè´£å‘é‡åŒ–å’Œç´¢å¼•ç”Ÿæˆ
- **è®­ç»ƒæ¨¡å¼**:
  - `chunk`: æ–‡æœ¬åˆ†å—
  - `qa`: é—®ç­”å¯¹
  - `image`: å›¾åƒå¤„ç†
  - `imageParse`: å›¾åƒè§£æ

## ä»£ç ç›®å½•ç»“æ„

### Packages å±‚(å…±äº«ä»£ç )

#### 1. packages/global/core/dataset/
**ç±»å‹å®šä¹‰å’Œå¸¸é‡**
```
â”œâ”€â”€ constants.ts              # æ‰€æœ‰æšä¸¾å®šä¹‰(ç±»å‹ã€çŠ¶æ€ã€æ¨¡å¼)
â”œâ”€â”€ type.d.ts                 # TypeScript ç±»å‹å®šä¹‰
â”œâ”€â”€ api.d.ts                  # API æ¥å£ç±»å‹
â”œâ”€â”€ controller.d.ts           # æ§åˆ¶å™¨ç±»å‹å®šä¹‰
â”œâ”€â”€ utils.ts                  # é€šç”¨å·¥å…·å‡½æ•°
â”œâ”€â”€ collection/
â”‚   â”œâ”€â”€ constants.ts          # é›†åˆç›¸å…³å¸¸é‡
â”‚   â””â”€â”€ utils.ts              # é›†åˆå·¥å…·å‡½æ•°
â”œâ”€â”€ data/
â”‚   â””â”€â”€ constants.ts          # æ•°æ®ç›¸å…³å¸¸é‡
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ type.d.ts             # è®­ç»ƒç›¸å…³ç±»å‹
â”‚   â””â”€â”€ utils.ts              # è®­ç»ƒå·¥å…·å‡½æ•°
â”œâ”€â”€ apiDataset/
â”‚   â”œâ”€â”€ type.d.ts             # APIæ•°æ®é›†ç±»å‹
â”‚   â””â”€â”€ utils.ts              # APIæ•°æ®é›†å·¥å…·
â””â”€â”€ search/
    â””â”€â”€ utils.ts              # æœç´¢å·¥å…·å‡½æ•°
```

**å…³é”®æšä¸¾å®šä¹‰**:
- `DatasetTypeEnum`: çŸ¥è¯†åº“ç±»å‹
- `DatasetCollectionTypeEnum`: é›†åˆç±»å‹
- `DatasetSearchModeEnum`: æœç´¢æ¨¡å¼(embedding/fullText/mixed)
- `TrainingModeEnum`: è®­ç»ƒæ¨¡å¼
- `DatasetCollectionDataProcessModeEnum`: æ•°æ®å¤„ç†æ¨¡å¼

#### 2. packages/service/core/dataset/
**ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®åº“æ“ä½œ**
```
â”œâ”€â”€ schema.ts                 # Dataset MongoDB Schema
â”œâ”€â”€ controller.ts             # Dataset æ ¸å¿ƒæ§åˆ¶å™¨
â”œâ”€â”€ utils.ts                  # ä¸šåŠ¡å·¥å…·å‡½æ•°
â”œâ”€â”€ collection/
â”‚   â”œâ”€â”€ schema.ts             # Collection Schema
â”‚   â”œâ”€â”€ controller.ts         # Collection æ§åˆ¶å™¨
â”‚   â””â”€â”€ utils.ts              # Collection å·¥å…·
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ schema.ts             # DatasetData Schema
â”‚   â”œâ”€â”€ dataTextSchema.ts     # å…¨æ–‡æœç´¢ Schema
â”‚   â””â”€â”€ controller.ts         # Data æ§åˆ¶å™¨
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ schema.ts             # Training Schema
â”‚   â”œâ”€â”€ controller.ts         # Training æ§åˆ¶å™¨
â”‚   â””â”€â”€ constants.ts          # Training å¸¸é‡
â”œâ”€â”€ tag/
â”‚   â””â”€â”€ schema.ts             # Tag Schema
â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ schema.ts             # Image Schema
â”‚   â””â”€â”€ utils.ts              # Image å·¥å…·
â”œâ”€â”€ search/
â”‚   â”œâ”€â”€ controller.ts         # ğŸ”¥ æ ¸å¿ƒæ£€ç´¢æ§åˆ¶å™¨
â”‚   â””â”€â”€ utils.ts              # æ£€ç´¢å·¥å…·å‡½æ•°
â””â”€â”€ apiDataset/
    â”œâ”€â”€ index.ts              # APIæ•°æ®é›†å…¥å£
    â”œâ”€â”€ custom/api.ts         # è‡ªå®šä¹‰API
    â”œâ”€â”€ feishuDataset/api.ts  # é£ä¹¦é›†æˆ
    â””â”€â”€ yuqueDataset/api.ts   # è¯­é›€é›†æˆ
```

### Projects å±‚(åº”ç”¨å®ç°)

#### 3. projects/app/src/pages/api/core/dataset/
**NextJS API è·¯ç”±**
```
â”œâ”€â”€ detail.ts                 # è·å–çŸ¥è¯†åº“è¯¦æƒ…
â”œâ”€â”€ delete.ts                 # åˆ é™¤çŸ¥è¯†åº“
â”œâ”€â”€ paths.ts                  # è·å–è·¯å¾„ä¿¡æ¯
â”œâ”€â”€ exportAll.ts              # å¯¼å‡ºå…¨éƒ¨æ•°æ®
â”œâ”€â”€ collection/
â”‚   â”œâ”€â”€ create.ts             # åˆ›å»ºé›†åˆ(åŸºç¡€)
â”‚   â”œâ”€â”€ create/
â”‚   â”‚   â”œâ”€â”€ localFile.ts      # æœ¬åœ°æ–‡ä»¶å¯¼å…¥
â”‚   â”‚   â”œâ”€â”€ link.ts           # é“¾æ¥å¯¼å…¥
â”‚   â”‚   â”œâ”€â”€ text.ts           # æ–‡æœ¬å¯¼å…¥
â”‚   â”‚   â”œâ”€â”€ images.ts         # å›¾ç‰‡å¯¼å…¥
â”‚   â”‚   â”œâ”€â”€ apiCollection.ts  # APIé›†åˆ
â”‚   â”‚   â””â”€â”€ fileId.ts         # æ–‡ä»¶IDå¯¼å…¥
â”‚   â”œâ”€â”€ update.ts             # æ›´æ–°é›†åˆ
â”‚   â”œâ”€â”€ list.ts               # é›†åˆåˆ—è¡¨
â”‚   â”œâ”€â”€ detail.ts             # é›†åˆè¯¦æƒ…
â”‚   â”œâ”€â”€ sync.ts               # åŒæ­¥é›†åˆ
â”‚   â””â”€â”€ export.ts             # å¯¼å‡ºé›†åˆ
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ list.ts               # æ•°æ®åˆ—è¡¨
â”‚   â”œâ”€â”€ detail.ts             # æ•°æ®è¯¦æƒ…
â”‚   â”œâ”€â”€ insertData.ts         # æ’å…¥æ•°æ®
â”‚   â”œâ”€â”€ pushData.ts           # æ¨é€æ•°æ®
â”‚   â”œâ”€â”€ update.ts             # æ›´æ–°æ•°æ®
â”‚   â””â”€â”€ delete.ts             # åˆ é™¤æ•°æ®
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ getDatasetTrainingQueue.ts        # è·å–è®­ç»ƒé˜Ÿåˆ—
â”‚   â”œâ”€â”€ getTrainingDataDetail.ts          # è®­ç»ƒæ•°æ®è¯¦æƒ…
â”‚   â”œâ”€â”€ updateTrainingData.ts             # æ›´æ–°è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ deleteTrainingData.ts             # åˆ é™¤è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ getTrainingError.ts               # è·å–è®­ç»ƒé”™è¯¯
â””â”€â”€ apiDataset/
    â”œâ”€â”€ list.ts               # APIæ•°æ®é›†åˆ—è¡¨
    â”œâ”€â”€ getCatalog.ts         # è·å–ç›®å½•
    â””â”€â”€ getPathNames.ts       # è·å–è·¯å¾„å
```

#### 4. projects/app/src/components/ å’Œ pageComponents/
**å‰ç«¯ç»„ä»¶**
```
components/core/dataset/      # é€šç”¨ç»„ä»¶
â”œâ”€â”€ SelectModal.tsx           # çŸ¥è¯†åº“é€‰æ‹©å™¨
â”œâ”€â”€ QuoteItem.tsx             # å¼•ç”¨é¡¹å±•ç¤º
â”œâ”€â”€ DatasetTypeTag.tsx        # ç±»å‹æ ‡ç­¾
â”œâ”€â”€ RawSourceBox.tsx          # åŸå§‹æ¥æºå±•ç¤º
â””â”€â”€ SearchParamsTip.tsx       # æœç´¢å‚æ•°æç¤º

pageComponents/dataset/       # é¡µé¢ç»„ä»¶
â”œâ”€â”€ list/                     # åˆ—è¡¨é¡µ
â”‚   â””â”€â”€ SideTag.tsx          # ä¾§è¾¹æ ‡ç­¾
â”œâ”€â”€ detail/                   # è¯¦æƒ…é¡µ
â”‚   â”œâ”€â”€ CollectionCard/      # é›†åˆå¡ç‰‡
â”‚   â”œâ”€â”€ DataCard.tsx         # æ•°æ®å¡ç‰‡
â”‚   â”œâ”€â”€ Test.tsx             # æµ‹è¯•ç»„ä»¶
â”‚   â”œâ”€â”€ Info/                # ä¿¡æ¯ç»„ä»¶
â”‚   â”œâ”€â”€ Import/              # å¯¼å…¥ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ diffSource/      # ä¸åŒæ•°æ®æº
â”‚   â”‚   â”œâ”€â”€ components/      # å…¬å…±ç»„ä»¶
â”‚   â”‚   â””â”€â”€ commonProgress/  # è¿›åº¦ç»„ä»¶
â”‚   â””â”€â”€ Form/                # è¡¨å•ç»„ä»¶
â””â”€â”€ ApiDatasetForm.tsx        # APIæ•°æ®é›†è¡¨å•
```

## æ•°æ®åº“ Schema è¯¦è§£

### 1. Dataset Schema (datasets é›†åˆ)
```typescript
{
  _id: ObjectId,
  parentId: ObjectId | null,          // çˆ¶çº§ID(æ”¯æŒæ–‡ä»¶å¤¹)
  teamId: ObjectId,                   // å›¢é˜ŸID
  tmbId: ObjectId,                    // å›¢é˜Ÿæˆå‘˜ID
  type: DatasetTypeEnum,              // çŸ¥è¯†åº“ç±»å‹
  avatar: string,                     // å¤´åƒ
  name: string,                       // åç§°
  intro: string,                      // ç®€ä»‹
  updateTime: Date,                   // æ›´æ–°æ—¶é—´

  vectorModel: string,                // å‘é‡æ¨¡å‹
  agentModel: string,                 // AIæ¨¡å‹
  vlmModel?: string,                  // è§†è§‰è¯­è¨€æ¨¡å‹

  websiteConfig?: {                   // ç½‘ç«™é…ç½®
    url: string,
    selector: string
  },

  chunkSettings: {                    // åˆ†å—é…ç½®
    trainingType: DatasetCollectionDataProcessModeEnum,
    chunkTriggerType: ChunkTriggerConfigTypeEnum,
    chunkTriggerMinSize: number,
    chunkSettingMode: ChunkSettingModeEnum,
    chunkSplitMode: DataChunkSplitModeEnum,
    chunkSize: number,
    chunkSplitter: string,
    indexSize: number,
    qaPrompt: string,
    // ... æ›´å¤šé…ç½®
  },

  inheritPermission: boolean,         // ç»§æ‰¿æƒé™
  apiDatasetServer?: object          // APIæœåŠ¡å™¨é…ç½®
}

// ç´¢å¼•
teamId_1
type_1
```

### 2. DatasetCollection Schema (dataset_collections é›†åˆ)
```typescript
{
  _id: ObjectId,
  parentId: ObjectId | null,          // çˆ¶çº§é›†åˆ
  teamId: ObjectId,
  tmbId: ObjectId,
  datasetId: ObjectId,                // æ‰€å±çŸ¥è¯†åº“

  type: DatasetCollectionTypeEnum,    // é›†åˆç±»å‹
  name: string,                       // åç§°
  tags: string[],                     // æ ‡ç­¾IDåˆ—è¡¨

  createTime: Date,
  updateTime: Date,

  // å…ƒæ•°æ®(æ ¹æ®ç±»å‹ä¸åŒ)
  fileId?: ObjectId,                  // æœ¬åœ°æ–‡ä»¶ID
  rawLink?: string,                   // åŸå§‹é“¾æ¥
  apiFileId?: string,                 // APIæ–‡ä»¶ID
  externalFileId?: string,            // å¤–éƒ¨æ–‡ä»¶ID
  externalFileUrl?: string,           // å¤–éƒ¨å¯¼å…¥URL

  rawTextLength?: number,             // åŸå§‹æ–‡æœ¬é•¿åº¦
  hashRawText?: string,               // æ–‡æœ¬å“ˆå¸Œ
  metadata?: object,                  // å…¶ä»–å…ƒæ•°æ®

  forbid: boolean,                    // æ˜¯å¦ç¦ç”¨

  // è§£æé…ç½®
  customPdfParse?: boolean,
  apiFileParentId?: string,

  // åˆ†å—é…ç½®(ç»§æ‰¿è‡ª ChunkSettings)
  ...chunkSettings
}

// ç´¢å¼•
teamId_1_fileId_1
teamId_1_datasetId_1_parentId_1_updateTime_-1
teamId_1_datasetId_1_tags_1
teamId_1_datasetId_1_createTime_1
datasetId_1_externalFileId_1 (unique)
```

### 3. DatasetData Schema (dataset_datas é›†åˆ)
```typescript
{
  _id: ObjectId,
  teamId: ObjectId,
  tmbId: ObjectId,
  datasetId: ObjectId,
  collectionId: ObjectId,

  q: string,                          // é—®é¢˜/å¤§å—æ–‡æœ¬
  a?: string,                         // ç­”æ¡ˆ/è‡ªå®šä¹‰å†…å®¹
  imageId?: string,                   // å›¾ç‰‡ID
  imageDescMap?: object,              // å›¾ç‰‡æè¿°æ˜ å°„

  updateTime: Date,
  chunkIndex: number,                 // å—ç´¢å¼•

  indexes: [{                         // å‘é‡ç´¢å¼•æ•°ç»„
    type: DatasetDataIndexTypeEnum,
    dataId: string,                   // PGå‘é‡æ•°æ®ID
    text: string                      // ç´¢å¼•æ–‡æœ¬
  }],

  history?: [{                        // å†å²ç‰ˆæœ¬
    q: string,
    a?: string,
    updateTime: Date
  }],

  rebuilding?: boolean                // é‡å»ºä¸­æ ‡å¿—
}

// ç´¢å¼•
teamId_1_datasetId_1_collectionId_1_chunkIndex_1_updateTime_-1
teamId_1_datasetId_1_collectionId_1_indexes.dataId_1
rebuilding_1_teamId_1_datasetId_1
```

### 4. DatasetTraining Schema (dataset_trainings é›†åˆ)
```typescript
{
  _id: ObjectId,
  teamId: ObjectId,
  tmbId: ObjectId,
  datasetId: ObjectId,
  collectionId: ObjectId,
  billId?: string,                    // è´¦å•ID

  mode: TrainingModeEnum,             // è®­ç»ƒæ¨¡å¼

  expireAt: Date,                     // è¿‡æœŸæ—¶é—´(7å¤©è‡ªåŠ¨åˆ é™¤)
  lockTime: Date,                     // é”å®šæ—¶é—´
  retryCount: number,                 // é‡è¯•æ¬¡æ•°

  q: string,                          // å¾…è®­ç»ƒé—®é¢˜
  a: string,                          // å¾…è®­ç»ƒç­”æ¡ˆ
  imageId?: string,
  imageDescMap?: object,
  chunkIndex: number,
  indexSize?: number,
  weight: number,                     // æƒé‡

  dataId?: ObjectId,                  // å…³è”çš„DatasetData ID

  indexes: [{                         // å¾…ç”Ÿæˆçš„ç´¢å¼•
    type: DatasetDataIndexTypeEnum,
    text: string
  }],

  errorMsg?: string                   // é”™è¯¯ä¿¡æ¯
}

// ç´¢å¼•
teamId_1_datasetId_1
mode_1_retryCount_1_lockTime_1_weight_-1
expireAt_1 (TTL: 7 days)
```

### 5. è¾…åŠ© Schema

#### DatasetCollectionTags (dataset_collection_tags)
```typescript
{
  _id: ObjectId,
  teamId: ObjectId,
  datasetId: ObjectId,
  tag: string                         // æ ‡ç­¾åç§°
}
```

#### DatasetDataText (dataset_data_texts) - å…¨æ–‡æœç´¢
```typescript
{
  _id: ObjectId,
  teamId: ObjectId,
  datasetId: ObjectId,
  collectionId: ObjectId,
  dataId: ObjectId,                   // å…³è” DatasetData
  fullTextToken: string               // å…¨æ–‡æœç´¢Token
}

// å…¨æ–‡ç´¢å¼•
fullTextToken: text
```

## æ ¸å¿ƒä¸šåŠ¡æµç¨‹

### 1. æ•°æ®å¯¼å…¥æµç¨‹

```
ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶/é“¾æ¥
    â†“
åˆ›å»º DatasetCollection
    â†“
æ–‡ä»¶è§£æ & é¢„å¤„ç†
    â†“
æ–‡æœ¬åˆ†å—(æ ¹æ® ChunkSettings)
    â†“
åˆ›å»º DatasetTraining ä»»åŠ¡
    â†“
åå°é˜Ÿåˆ—å¤„ç†:
  - å‘é‡åŒ–(embedding)
  - åˆ›å»º PG å‘é‡ç´¢å¼•
  - ç”Ÿæˆ DatasetData
  - åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•(DatasetDataText)
    â†“
è®­ç»ƒå®Œæˆ,å¯ä»¥æ£€ç´¢
```

**å…³é”®ä»£ç ä½ç½®**:
- æ–‡ä»¶ä¸Šä¼ : `projects/app/src/pages/api/core/dataset/collection/create/localFile.ts`
- åˆ†å—é€»è¾‘: `packages/service/core/dataset/collection/utils.ts`
- è®­ç»ƒæ§åˆ¶: `packages/service/core/dataset/training/controller.ts`

### 2. æ£€ç´¢æµç¨‹(æ ¸å¿ƒç®—æ³•)

**ä½ç½®**: `packages/service/core/dataset/search/controller.ts`

```typescript
// ä¸‰ç§æ£€ç´¢æ¨¡å¼
enum DatasetSearchModeEnum {
  embedding = 'embedding',        // çº¯å‘é‡æ£€ç´¢
  fullTextRecall = 'fullTextRecall', // çº¯å…¨æ–‡æ£€ç´¢
  mixedRecall = 'mixedRecall'     // æ··åˆæ£€ç´¢
}

// æ£€ç´¢æµç¨‹
async function searchDatasetData(props) {
  // 1. å‚æ•°åˆå§‹åŒ–å’Œæƒé‡é…ç½®
  const { embeddingWeight, rerankWeight } = props;

  // 2. é›†åˆè¿‡æ»¤(æ ‡ç­¾/æ—¶é—´/ç¦ç”¨)
  const filterCollectionIds = await filterCollectionByMetadata();

  // 3. å¤šè·¯å¬å›
  const { embeddingRecallResults, fullTextRecallResults } =
    await multiQueryRecall({
      embeddingLimit: 80,  // å‘é‡å¬å›æ•°é‡
      fullTextLimit: 60    // å…¨æ–‡å¬å›æ•°é‡
    });

  // 4. RRF(å€’æ•°æ’åèåˆ)åˆå¹¶
  const rrfResults = datasetSearchResultConcat([
    { weight: embeddingWeight, list: embeddingRecallResults },
    { weight: 1 - embeddingWeight, list: fullTextRecallResults }
  ]);

  // 5. ReRank é‡æ’åº(å¯é€‰)
  if (usingReRank) {
    const reRankResults = await datasetDataReRank({
      rerankModel,
      query: reRankQuery,
      data: rrfResults
    });
  }

  // 6. ç›¸ä¼¼åº¦è¿‡æ»¤
  const scoreFiltered = results.filter(item =>
    item.score >= similarity
  );

  // 7. Token é™åˆ¶è¿‡æ»¤
  const finalResults = await filterDatasetDataByMaxTokens(
    scoreFiltered,
    maxTokens
  );

  return finalResults;
}
```

**æ ¸å¿ƒç®—æ³•è¯¦è§£**:

#### a. å‘é‡å¬å› (embeddingRecall)
```typescript
// 1. æŸ¥è¯¢å‘é‡åŒ–
const { vectors, tokens } = await getVectorsByText({
  model: getEmbeddingModel(model),
  input: queries,
  type: 'query'
});

// 2. PG å‘é‡åº“å¬å›
const recallResults = await Promise.all(
  vectors.map(vector =>
    recallFromVectorStore({
      teamId,
      datasetIds,
      vector,
      limit,
      forbidCollectionIdList,
      filterCollectionIdList
    })
  )
);

// 3. å…³è” MongoDB æ•°æ®
const dataMaps = await MongoDatasetData.find({
  teamId,
  datasetId: { $in: datasetIds },
  'indexes.dataId': { $in: indexDataIds }
});
```

#### b. å…¨æ–‡å¬å› (fullTextRecall)
```typescript
// MongoDB å…¨æ–‡æœç´¢
const results = await MongoDatasetDataText.aggregate([
  {
    $match: {
      teamId: new Types.ObjectId(teamId),
      $text: { $search: await jiebaSplit({ text: query }) },
      datasetId: { $in: datasetIds.map(id => new Types.ObjectId(id)) }
    }
  },
  {
    $sort: {
      score: { $meta: 'textScore' }
    }
  },
  {
    $limit: limit
  }
]);
```

#### c. RRF åˆå¹¶ç®—æ³•
```typescript
// å€’æ•°æ’åèåˆ(Reciprocal Rank Fusion)
function datasetSearchResultConcat(weightedLists) {
  const k = 60; // RRF å‚æ•°
  const scoreMap = new Map();

  for (const { weight, list } of weightedLists) {
    list.forEach((item, index) => {
      const rrfScore = weight / (k + index + 1);
      scoreMap.set(item.id,
        (scoreMap.get(item.id) || 0) + rrfScore
      );
    });
  }

  return Array.from(scoreMap.entries())
    .sort((a, b) => b[1] - a[1])
    .map(([id]) => findItemById(id));
}
```

#### d. ReRank é‡æ’åº
```typescript
// ä½¿ç”¨é‡æ’åºæ¨¡å‹(å¦‚ bge-reranker)
const { results } = await reRankRecall({
  model: rerankModel,
  query: reRankQuery,
  documents: data.map(item => ({
    id: item.id,
    text: `${item.q}\n${item.a}`
  }))
});

// é‡æ’åºç»“æœèåˆåˆ° RRF ç»“æœ
const finalResults = datasetSearchResultConcat([
  { weight: 1 - rerankWeight, list: rrfResults },
  { weight: rerankWeight, list: reRankResults }
]);
```

### 3. åˆ†å—ç­–ç•¥

**ä½ç½®**: `packages/global/core/dataset/constants.ts`

```typescript
// åˆ†å—æ¨¡å¼
enum DataChunkSplitModeEnum {
  paragraph = 'paragraph',  // æ®µè½åˆ†å‰²(æ™ºèƒ½)
  size = 'size',           // å›ºå®šå¤§å°åˆ†å‰²
  char = 'char'            // å­—ç¬¦åˆ†éš”ç¬¦åˆ†å‰²
}

// AI æ®µè½æ¨¡å¼
enum ParagraphChunkAIModeEnum {
  auto = 'auto',           // è‡ªåŠ¨åˆ¤æ–­
  force = 'force',         // å¼ºåˆ¶ä½¿ç”¨AI
  forbid = 'forbid'        // ç¦ç”¨AI
}

// åˆ†å—é…ç½®ç¤ºä¾‹
const chunkSettings = {
  chunkSplitMode: 'paragraph',
  chunkSize: 512,           // æœ€å¤§å—å¤§å°
  chunkSplitter: '\n',      // åˆ†éš”ç¬¦
  paragraphChunkDeep: 2,    // æ®µè½å±‚çº§
  paragraphChunkMinSize: 100, // æœ€å°æ®µè½å¤§å°
  indexSize: 256,           // ç´¢å¼•å¤§å°
  // æ•°æ®å¢å¼º
  dataEnhanceCollectionName: true,
  autoIndexes: true,        // è‡ªåŠ¨å¤šç´¢å¼•
  indexPrefixTitle: true    // ç´¢å¼•å‰ç¼€æ ‡é¢˜
}
```

### 4. è®­ç»ƒé˜Ÿåˆ—æœºåˆ¶

**ä½ç½®**: `packages/service/core/dataset/training/controller.ts`

```typescript
// è®­ç»ƒé˜Ÿåˆ—è°ƒåº¦
class TrainingQueue {
  // 1. è·å–å¾…è®­ç»ƒä»»åŠ¡(æŒ‰æƒé‡æ’åº)
  async getNextTrainingTask() {
    return MongoDatasetTraining.findOne({
      mode: { $in: supportedModes },
      retryCount: { $gt: 0 },
      lockTime: { $lt: new Date(Date.now() - lockTimeout) }
    })
    .sort({ weight: -1, lockTime: 1 })
    .limit(1);
  }

  // 2. é”å®šä»»åŠ¡
  async lockTask(taskId) {
    await MongoDatasetTraining.updateOne(
      { _id: taskId },
      { $set: { lockTime: new Date() } }
    );
  }

  // 3. æ‰§è¡Œå‘é‡åŒ–
  async processTask(task) {
    const vectors = await getVectorsByText({
      model: getEmbeddingModel(task.model),
      input: task.indexes.map(i => i.text)
    });

    // ä¿å­˜åˆ° PG å‘é‡åº“
    const indexDataIds = await saveToVectorDB(vectors);

    // åˆ›å»º DatasetData
    await MongoDatasetData.create({
      ...task,
      indexes: task.indexes.map((idx, i) => ({
        ...idx,
        dataId: indexDataIds[i]
      }))
    });
  }

  // 4. å®Œæˆ/å¤±è´¥å¤„ç†
  async completeTask(taskId, success, error) {
    if (success) {
      await MongoDatasetTraining.deleteOne({ _id: taskId });
    } else {
      await MongoDatasetTraining.updateOne(
        { _id: taskId },
        {
          $inc: { retryCount: -1 },
          $set: {
            errorMsg: error,
            lockTime: new Date('2000/1/1')
          }
        }
      );
    }
  }
}
```

## å…³é”®æŠ€æœ¯ç‚¹

### 1. å¤šç´¢å¼•æœºåˆ¶

**ä¸ºä»€ä¹ˆéœ€è¦å¤šç´¢å¼•?**
- å¤§å—æ–‡æœ¬å¯ä»¥æ‹†åˆ†ä¸ºå¤šä¸ªå°ç´¢å¼•,æé«˜å¬å›ç²¾åº¦
- æ”¯æŒä¸åŒç²’åº¦çš„æ£€ç´¢(ç²—ç²’åº¦+ç»†ç²’åº¦)

```typescript
// DatasetData ä¸­çš„ indexes æ•°ç»„
{
  q: "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬...",
  indexes: [
    {
      type: 'custom',      // è‡ªå®šä¹‰ç´¢å¼•
      dataId: 'pg_vector_id_1',
      text: "ç¬¬ä¸€éƒ¨åˆ†ç´¢å¼•æ–‡æœ¬"
    },
    {
      type: 'custom',
      dataId: 'pg_vector_id_2',
      text: "ç¬¬äºŒéƒ¨åˆ†ç´¢å¼•æ–‡æœ¬"
    }
  ]
}
```

### 2. æ··åˆæ£€ç´¢(Hybrid Search)

**ç»“åˆå‘é‡æ£€ç´¢å’Œå…¨æ–‡æ£€ç´¢çš„ä¼˜åŠ¿**:
- å‘é‡æ£€ç´¢: è¯­ä¹‰ç›¸ä¼¼åº¦,ç†è§£æ„å›¾
- å…¨æ–‡æ£€ç´¢: ç²¾ç¡®åŒ¹é…å…³é”®è¯,é«˜å¬å›
- RRF èåˆ: äº’è¡¥ä¼˜åŠ¿,æå‡æ•´ä½“æ•ˆæœ

**æƒé‡é…ç½®**:
```typescript
{
  searchMode: 'mixedRecall',
  embeddingWeight: 0.5,      // å‘é‡æƒé‡
  // fullTextWeight = 1 - 0.5 = 0.5

  usingReRank: true,
  rerankWeight: 0.7          // é‡æ’åºæƒé‡
}
```

### 3. é›†åˆè¿‡æ»¤(Collection Filter)

**æ”¯æŒçµæ´»çš„å…ƒæ•°æ®è¿‡æ»¤**:
```typescript
// æ ‡ç­¾è¿‡æ»¤
{
  tags: {
    $and: ["æ ‡ç­¾1", "æ ‡ç­¾2"],  // å¿…é¡»åŒæ—¶åŒ…å«
    $or: ["æ ‡ç­¾3", "æ ‡ç­¾4", null] // åŒ…å«ä»»ä¸€,nullè¡¨ç¤ºæ— æ ‡ç­¾
  }
}

// æ—¶é—´è¿‡æ»¤
{
  createTime: {
    $gte: '2024-01-01',
    $lte: '2024-12-31'
  }
}
```

### 4. å‘é‡æ•°æ®åº“æ¶æ„

**åŒæ•°æ®åº“æ¶æ„**:
```
MongoDB (å…ƒæ•°æ® + å…¨æ–‡ç´¢å¼•)
  - å­˜å‚¨åŸå§‹æ–‡æœ¬ã€é…ç½®ã€å…³ç³»
  - å…¨æ–‡æœç´¢ç´¢å¼•(jieba åˆ†è¯)

PostgreSQL + pgvector (å‘é‡å­˜å‚¨)
  - é«˜ç»´å‘é‡å­˜å‚¨
  - é«˜æ•ˆä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
  - HNSW ç´¢å¼•åŠ é€Ÿ
```

**æ•°æ®æµè½¬**:
```
åŸå§‹æ–‡æœ¬ â†’ Embedding API â†’ å‘é‡ â†’ PG å­˜å‚¨
         â†“
         ç´¢å¼•ID å­˜å› MongoDB

æ£€ç´¢æ—¶:
æŸ¥è¯¢æ–‡æœ¬ â†’ å‘é‡ â†’ PG å¬å› topK â†’
è·å– dataIds â†’ MongoDB æŸ¥è¯¢å®Œæ•´æ•°æ®
```

### 5. å›¾ç‰‡çŸ¥è¯†åº“

**ç‰¹æ®Šçš„å›¾ç‰‡å¤„ç†æµç¨‹**:
```typescript
// 1. å›¾ç‰‡ä¸Šä¼ 
{
  type: 'images',
  imageId: 'image_storage_id'
}

// 2. å›¾ç‰‡å‘é‡åŒ–(VLM)
const imageVector = await getImageEmbedding({
  model: vlmModel,
  imageId
});

// 3. å›¾ç‰‡æè¿°æ˜ å°„
{
  imageDescMap: {
    'image_url_1': 'è¿™æ˜¯ä¸€å¼ äº§å“å›¾ç‰‡',
    'image_url_2': 'è¿™æ˜¯ä¸€å¼ æµç¨‹å›¾'
  }
}

// 4. æ£€ç´¢æ—¶è¿”å›é¢„ç­¾åURL
const previewUrl = getDatasetImagePreviewUrl({
  imageId,
  teamId,
  datasetId,
  expiredMinutes: 60 * 24 * 7  // 7å¤©æœ‰æ•ˆ
});
```

## API è·¯ç”±æ˜ å°„

### Dataset åŸºç¡€æ“ä½œ
```
GET    /api/core/dataset/detail        # è·å–çŸ¥è¯†åº“è¯¦æƒ…
DELETE /api/core/dataset/delete        # åˆ é™¤çŸ¥è¯†åº“
GET    /api/core/dataset/paths         # è·å–è·¯å¾„
POST   /api/core/dataset/exportAll     # å¯¼å‡ºå…¨éƒ¨
```

### Collection æ“ä½œ
```
POST   /api/core/dataset/collection/create              # åˆ›å»ºé›†åˆ
POST   /api/core/dataset/collection/create/localFile    # æœ¬åœ°æ–‡ä»¶
POST   /api/core/dataset/collection/create/link         # é“¾æ¥å¯¼å…¥
POST   /api/core/dataset/collection/create/text         # æ–‡æœ¬å¯¼å…¥
POST   /api/core/dataset/collection/create/images       # å›¾ç‰‡å¯¼å…¥
PUT    /api/core/dataset/collection/update              # æ›´æ–°é›†åˆ
GET    /api/core/dataset/collection/list                # é›†åˆåˆ—è¡¨
GET    /api/core/dataset/collection/detail              # é›†åˆè¯¦æƒ…
POST   /api/core/dataset/collection/sync                # åŒæ­¥é›†åˆ
GET    /api/core/dataset/collection/export              # å¯¼å‡ºé›†åˆ
```

### Data æ“ä½œ
```
GET    /api/core/dataset/data/list         # æ•°æ®åˆ—è¡¨
GET    /api/core/dataset/data/detail       # æ•°æ®è¯¦æƒ…
POST   /api/core/dataset/data/insertData   # æ’å…¥æ•°æ®
POST   /api/core/dataset/data/pushData     # æ¨é€æ•°æ®(æ‰¹é‡)
PUT    /api/core/dataset/data/update       # æ›´æ–°æ•°æ®
DELETE /api/core/dataset/data/delete       # åˆ é™¤æ•°æ®
```

### Training æ“ä½œ
```
GET    /api/core/dataset/training/getDatasetTrainingQueue   # è®­ç»ƒé˜Ÿåˆ—
GET    /api/core/dataset/training/getTrainingDataDetail     # è®­ç»ƒè¯¦æƒ…
PUT    /api/core/dataset/training/updateTrainingData        # æ›´æ–°è®­ç»ƒ
DELETE /api/core/dataset/training/deleteTrainingData        # åˆ é™¤è®­ç»ƒ
GET    /api/core/dataset/training/getTrainingError          # è·å–é”™è¯¯
```

## å‰ç«¯çŠ¶æ€ç®¡ç†

**ä½ç½®**: `projects/app/src/web/core/dataset/store/`

```typescript
// dataset.ts - çŸ¥è¯†åº“çŠ¶æ€
{
  datasets: DatasetListItemType[],
  currentDataset: DatasetItemType,
  loadDatasets: () => Promise<void>,
  createDataset: (data) => Promise<string>,
  updateDataset: (data) => Promise<void>,
  deleteDataset: (id) => Promise<void>
}

// searchTest.ts - æœç´¢æµ‹è¯•çŠ¶æ€
{
  searchQuery: string,
  searchMode: DatasetSearchModeEnum,
  similarity: number,
  limit: number,
  searchResults: SearchDataResponseItemType[],
  performSearch: () => Promise<void>
}
```

## æ€§èƒ½ä¼˜åŒ–è¦ç‚¹

### 1. ç´¢å¼•ä¼˜åŒ–
```javascript
// æ ¸å¿ƒå¤åˆç´¢å¼•
DatasetCollection:
  - { teamId: 1, datasetId: 1, parentId: 1, updateTime: -1 }
  - { teamId: 1, datasetId: 1, tags: 1 }

DatasetData:
  - { teamId: 1, datasetId: 1, collectionId: 1, chunkIndex: 1, updateTime: -1 }
  - { teamId: 1, datasetId: 1, collectionId: 1, 'indexes.dataId': 1 }

DatasetTraining:
  - { mode: 1, retryCount: 1, lockTime: 1, weight: -1 }
```

### 2. æŸ¥è¯¢ä¼˜åŒ–
```typescript
// ä½¿ç”¨ä»åº“è¯»å–(é™ä½ä¸»åº“å‹åŠ›)
const readFromSecondary = {
  readPreference: 'secondaryPreferred'
};

MongoDatasetData.find(query, fields, {
  ...readFromSecondary
}).lean();
```

### 3. åˆ†é¡µä¼˜åŒ–
```typescript
// ä½¿ç”¨ scrollList è€Œéä¼ ç»Ÿåˆ†é¡µ
// é¿å…æ·±åº¦åˆ†é¡µæ€§èƒ½é—®é¢˜
GET /api/core/dataset/collection/scrollList?lastId=xxx&limit=20
```

### 4. ç¼“å­˜ç­–ç•¥
```typescript
// Redis ç¼“å­˜çƒ­é—¨æ£€ç´¢ç»“æœ
const cacheKey = `dataset:search:${hashQuery(query)}`;
const cached = await redis.get(cacheKey);
if (cached) return JSON.parse(cached);

// ç¼“å­˜ 5 åˆ†é’Ÿ
await redis.setex(cacheKey, 300, JSON.stringify(results));
```

## æµ‹è¯•è¦†ç›–

**æµ‹è¯•æ–‡ä»¶ä½ç½®**: `projects/app/test/api/core/dataset/`

```
â”œâ”€â”€ create.test.ts                    # çŸ¥è¯†åº“åˆ›å»º
â”œâ”€â”€ paths.test.ts                     # è·¯å¾„æµ‹è¯•
â”œâ”€â”€ collection/
â”‚   â””â”€â”€ paths.test.ts                 # é›†åˆè·¯å¾„
â””â”€â”€ training/
    â”œâ”€â”€ deleteTrainingData.test.ts    # è®­ç»ƒåˆ é™¤
    â”œâ”€â”€ getTrainingError.test.ts      # è®­ç»ƒé”™è¯¯
    â””â”€â”€ updateTrainingData.test.ts    # è®­ç»ƒæ›´æ–°
```

## å¸¸è§å¼€å‘ä»»åŠ¡

### 1. æ·»åŠ æ–°çš„æ•°æ®æºç±»å‹

**æ­¥éª¤**:
1. åœ¨ `packages/global/core/dataset/constants.ts` æ·»åŠ æ–°ç±»å‹æšä¸¾
2. åœ¨ `packages/service/core/dataset/apiDataset/` åˆ›å»ºæ–°é›†æˆ
3. åœ¨ `projects/app/src/pages/api/core/dataset/collection/create/` æ·»åŠ  API è·¯ç”±
4. åœ¨ `projects/app/src/pageComponents/dataset/detail/Import/diffSource/` æ·»åŠ å‰ç«¯ç»„ä»¶

### 2. ä¿®æ”¹æ£€ç´¢ç®—æ³•

**æ ¸å¿ƒæ–‡ä»¶**: `packages/service/core/dataset/search/controller.ts`

å…³é”®å‡½æ•°:
- `embeddingRecall`: å‘é‡å¬å›é€»è¾‘
- `fullTextRecall`: å…¨æ–‡å¬å›é€»è¾‘
- `datasetSearchResultConcat`: RRF èåˆç®—æ³•
- `datasetDataReRank`: é‡æ’åºé€»è¾‘

### 3. ä¼˜åŒ–åˆ†å—ç­–ç•¥

**æ ¸å¿ƒæ–‡ä»¶**: `packages/service/core/dataset/collection/utils.ts`

å…³é”®é€»è¾‘:
- æ®µè½è¯†åˆ«
- æ™ºèƒ½åˆå¹¶å°å—
- æ ‡é¢˜æå–
- å¤šç´¢å¼•ç”Ÿæˆ

### 4. æ·»åŠ æ–°çš„è®­ç»ƒæ¨¡å¼

**æ­¥éª¤**:
1. åœ¨ `TrainingModeEnum` æ·»åŠ æ–°æ¨¡å¼
2. åœ¨ `packages/service/core/dataset/training/controller.ts` æ·»åŠ å¤„ç†é€»è¾‘
3. æ›´æ–°è®­ç»ƒé˜Ÿåˆ—è°ƒåº¦å™¨

## ä¾èµ–å…³ç³»å›¾

```
Dataset (1:N)
  â”œâ”€â†’ DatasetCollection (1:N)
  â”‚     â”œâ”€â†’ DatasetData (1:N)
  â”‚     â”‚     â””â”€â†’ PG Vectors (1:N)
  â”‚     â””â”€â†’ DatasetTraining (1:N)
  â”‚           â””â”€â†’ Bills (1:1)
  â””â”€â†’ DatasetCollectionTags (1:N)
        â””â”€â†’ DatasetCollection.tags[] (N:M)
```

## æƒé™ç³»ç»Ÿ

**ä½ç½®**: `packages/global/support/permission/dataset/`

```typescript
// æƒé™çº§åˆ«
enum PermissionTypeEnum {
  owner = 'owner',         // æ‰€æœ‰è€…
  manage = 'manage',       // ç®¡ç†å‘˜
  write = 'write',         // ç¼–è¾‘
  read = 'read'            // åªè¯»
}

// æƒé™ç»§æ‰¿
{
  inheritPermission: true  // ä»çˆ¶çº§ç»§æ‰¿æƒé™
}

// åä½œè€…ç®¡ç†
DatasetCollaborators: {
  datasetId,
  tmbId,
  permission: PermissionTypeEnum
}
```

## å›½é™…åŒ–

**ä½ç½®**: `packages/web/i18n/`

```typescript
// çŸ¥è¯†åº“ç›¸å…³ç¿»è¯‘ key
'dataset:common_dataset'
'dataset:folder_dataset'
'dataset:website_dataset'
'dataset:api_file'
'dataset:sync_collection_failed'
'dataset:training.Image mode'
// ... æ›´å¤š
```

## è°ƒè¯•æŠ€å·§

### 1. æŸ¥çœ‹è®­ç»ƒé˜Ÿåˆ—çŠ¶æ€
```javascript
// MongoDB Shell
db.dataset_trainings.find({
  teamId: ObjectId('xxx')
}).sort({ weight: -1, lockTime: 1 }).limit(10)
```

### 2. æ£€æŸ¥å‘é‡ç´¢å¼•
```javascript
// PG SQL
SELECT datasetid, count(*)
FROM pg_vectors
GROUP BY datasetid;
```

### 3. å…¨æ–‡æœç´¢æµ‹è¯•
```javascript
db.dataset_data_texts.find({
  $text: { $search: "æµ‹è¯•æŸ¥è¯¢" }
}, {
  score: { $meta: "textScore" }
}).sort({ score: { $meta: "textScore" } })
```

### 4. æŸ¥çœ‹æ£€ç´¢æ—¥å¿—
```typescript
// å¼€å¯è¯¦ç»†æ—¥å¿—
searchDatasetData({
  ...props,
  debug: true  // è¾“å‡ºè¯¦ç»†å¬å›ä¿¡æ¯
})
```

## æœ€ä½³å®è·µ

### 1. åˆ†å—å¤§å°è®¾ç½®
- **çŸ­æ–‡æ¡£**: `chunkSize: 256-512`
- **é•¿æ–‡æ¡£**: `chunkSize: 512-1024`
- **FAQ**: `chunkSize: 128-256`

### 2. æ£€ç´¢å‚æ•°è°ƒä¼˜
```typescript
// é«˜ç²¾åº¦åœºæ™¯(å®¢æœ)
{
  searchMode: 'mixedRecall',
  similarity: 0.7,           // è¾ƒé«˜é˜ˆå€¼
  embeddingWeight: 0.6,      // åå‘è¯­ä¹‰
  usingReRank: true,
  rerankWeight: 0.8
}

// é«˜å¬å›åœºæ™¯(æœç´¢)
{
  searchMode: 'mixedRecall',
  similarity: 0.4,           // è¾ƒä½é˜ˆå€¼
  embeddingWeight: 0.4,      // åå‘å…¨æ–‡
  usingReRank: false
}
```

### 3. æ ‡ç­¾ç»„ç»‡
```
æŒ‰ä¸»é¢˜: #äº§å“æ–‡æ¡£ #æŠ€æœ¯è§„èŒƒ #å®¢æœFAQ
æŒ‰æ¥æº: #å®˜ç½‘ #æ‰‹å†Œ #ç¤¾åŒº
æŒ‰æ—¶æ•ˆ: #2024Q1 #æœ€æ–°ç‰ˆæœ¬
```

### 4. æ€§èƒ½ç›‘æ§
```typescript
// å…³é”®æŒ‡æ ‡
- è®­ç»ƒé˜Ÿåˆ—é•¿åº¦
- æ£€ç´¢å¹³å‡è€—æ—¶
- Token æ¶ˆè€—é‡
- å‘é‡åº“å¤§å°
- å¬å›ç‡/å‡†ç¡®ç‡
```

## æ‰©å±•é˜…è¯»

### ç›¸å…³æ–‡æ¡£
- [RAG æ¶æ„è®¾è®¡](https://docs.tryfastgpt.ai/docs/development/upgrading/4819/)
- [å‘é‡æ•°æ®åº“é€‰æ‹©](https://docs.tryfastgpt.ai/docs/development/custom-models/vector/)
- [æ£€ç´¢ä¼˜åŒ–æŒ‡å—](https://docs.tryfastgpt.ai/docs/workflow/modules/knowledge_base/)

### å¤–éƒ¨ä¾èµ–
- `pgvector`: PostgreSQL å‘é‡æ‰©å±•
- `jieba`: ä¸­æ–‡åˆ†è¯åº“
- `tiktoken`: Token è®¡æ•°
- `pdf-parse`: PDF è§£æ
- `mammoth`: Word è§£æ

---

## æ€»ç»“

FastGPT çŸ¥è¯†åº“æ¨¡å—æ˜¯ä¸€ä¸ªå®Œæ•´çš„ RAG ç³»ç»Ÿå®ç°,æ ¸å¿ƒç‰¹ç‚¹:

1. **åˆ†å±‚æ¶æ„**: Dataset â†’ Collection â†’ Data â†’ Indexes
2. **æ··åˆæ£€ç´¢**: å‘é‡ + å…¨æ–‡ + é‡æ’åº,çµæ´»é…ç½®æƒé‡
3. **å¼‚æ­¥è®­ç»ƒ**: é˜Ÿåˆ—åŒ–å‘é‡åŒ–ä»»åŠ¡,æ”¯æŒé‡è¯•å’Œå¤±è´¥å¤„ç†
4. **åŒæ•°æ®åº“**: MongoDB å­˜å…ƒæ•°æ®,PG å­˜å‘é‡
5. **å¤šæ•°æ®æº**: æ”¯æŒæ–‡ä»¶/é“¾æ¥/API/å¤–éƒ¨é›†æˆ
6. **çµæ´»åˆ†å—**: æ®µè½/å¤§å°/å­—ç¬¦å¤šç§ç­–ç•¥
7. **æƒé™æ§åˆ¶**: ç»§æ‰¿å¼æƒé™ç®¡ç†

å¼€å‘æ—¶é‡ç‚¹å…³æ³¨:
- **æ£€ç´¢æ€§èƒ½**: `search/controller.ts`
- **åˆ†å—è´¨é‡**: `collection/utils.ts`
- **è®­ç»ƒé˜Ÿåˆ—**: `training/controller.ts`
- **æ•°æ®æµè½¬**: Schema ä¹‹é—´çš„å…³è”å…³ç³»
