{
  "Action": "Operation",
  "Evaluation_app": "App",
  "Evaluation_app_tip": "Supports a simple app or a workflow without user interaction nodes. Plugins are not supported.",
  "Evaluation_file": "File",
  "Evaluation_model": "Model",
  "Executor": "Operator",
  "Overall_score": "Score",
  "Progress": "Progress",
  "Start_end_time": "Start/End Time",
  "Task_name_placeholder": "",
  "answer": "Standard answer",
  "app_required": "Please select an app first.",
  "app_response": "App output",
  "back": "Exit",
  "check_error": "Verification failed.",
  "check_error_tip": "Please check the following:\n1. Headers in the file must be consistent with those in the CSV template.\n2. All required fields are configured.\n3. Global variables are valid.\n4. Numeric input boxes contain valid numbers.\n5. Radio button options are valid.",
  "check_format": "Format verification",
  "comfirm_delete_item": "Are you sure you want to delete this entry?",
  "comfirm_delete_task": "Are you sure you want to delete this task and all related data?",
  "completed": "Completed",
  "create_task": "Create task",
  "data": "Data",
  "data_list": "Details",
  "detail": "Details",
  "error": "Error",
  "error_tooltip": "A task encountered error.\nClick to view task details.",
  "eval_file_check_error": "Evaluation file verification failed.",
  "evaluating": "Evaluating",
  "evaluation": "App evaluation",
  "evaluation_export_title": "Question,Standard answer,Actual answer,Status,Score",
  "evaluation_file_max_size": "Entries: {{count}}",
  "export": "Export",
  "file_required": "Select",
  "file_uploading": "Uploading file: {{num}}%",
  "history": "History",
  "paused": "Paused",
  "question": "Question",
  "queuing": "Waiting",
  "search_task": "Task",
  "standard_response": "Standard output",
  "start_evaluation": "Evaluate",
  "stauts": "Status",
  "task_creating": "Creating task",
  "task_detail": "Task details",
  "task_name": "Task name",
  "team_has_running_evaluation": "An app evaluation task is in progress. Please wait until it is complete.",
  "template_csv_file_select_tip": "Must be a {{fileType}} file that <highlight>strictly follows the template</highlight>.",
  "variables": "Global variables",
  "all_apps": "All apps",
  "search_evaluation_task": "Task name, app version",
  "create_new_task": "Create task",
  "task_name_column": "Task name",
  "progress_column": "Progress",
  "evaluation_app_column": "App",
  "app_version_column": "App version",
  "evaluation_result_column": "Evaluation result",
  "start_finish_time_column": "Time started/completed",
  "executor_column": "Operator",
  "waiting": "Waiting",
  "evaluating_status": "Evaluating",
  "completed_status": "Completed",
  "queuing_status": "Waiting",
  "running_status": "Ongoing",
  "error_data_tooltip": "{{count}} entries encountered error during execution. Click to view details.",
  "rename": "Rename",
  "delete": "Delete",
  "confirm_delete_task": "Are you sure you want to delete this task?",
  "evaluation_tasks_tab": "Tasks",
  "evaluation_tasks": "Evaluation tasks",
  "evaluation_datasets_tab": "Datasets",
  "evaluation_dimensions_tab": "Metrics",
  "create_new": "Create",
  "retry_error_data": "Retry failed entries",
  "dataset_name_placeholder": "Name",
  "create_new_dataset": "Create",
  "smart_generation": "Auto generate",
  "file_import": "Import from file",
  "confirm_delete_dataset": "Are you sure you want to delete this dataset?",
  "error_details": "Details",
  "status_queuing": "Waiting",
  "status_parsing": "Parsing file",
  "status_generating": "Generating data",
  "status_generate_error": "Generation error",
  "status_ready": "Ready",
  "status_parse_error": "Parsing error",
  "click_to_view_details": "View details",
  "table_header_name": "Name",
  "table_header_data_count": "Data size",
  "table_header_time": "Time created/updated",
  "table_header_status": "Status",
  "table_header_creator": "Creator",
  "create_dimension": "Create",
  "search_dimension": "Metric",
  "delete_failed": "Operation failed.",
  "delete_success": "Deleted successfully.",
  "builtin": "Predefined",
  "confirm_delete_dimension": "Are you sure that you want to delete this metric?",
  "dimension_name": "Name",
  "description": "Description",
  "create_update_time": "Time created/updated",
  "creator": "Creator",
  "all": "All",
  "app": "App",
  "citation_template": "Template",
  "correctness": "Correctness",
  "conciseness": "Conciseness",
  "harmfulness": "Harmfulness",
  "controversiality": "Controversiality",
  "creativity": "Creativity",
  "criminality": "Criminality",
  "depth": "Depth",
  "details": "Detail",
  "dimension_name_label": "Name",
  "dimension_description_label": "Description",
  "prompt_label": "Prompt",
  "citation_template_button": "Template",
  "test_run_title": "Trial run",
  "question_label": "Question",
  "question_placeholder": "",
  "answer_label": "Answer",
  "reference_answer_label": "Answer for reference",
  "reference_answer_placeholder": "",
  "actual_answer_label": "Actual answer",
  "actual_answer_placeholder": "",
  "run_result_label": "Running result",
  "start_run_button": "Run",
  "running_text": "Running",
  "run_success": "Run successful",
  "run_failed": "Run failed",
  "not_run": "Not run",
  "score_unit": "(Score)",
  "error_info_label": "Error: ",
  "no_feedback_text": "No feedback available.",
  "dimension_create_back": "Exit",
  "dimension_create_test_run": "Trial run",
  "dimension_create_confirm": "OK",
  "dimension_create_success": "Created successfully.",
  "dimension_create_name_required": "Name is required.",
  "dimension_create_prompt_required": "Prompt is required.",
  "dimension_get_data_failed": "Failed to obtain the metric data.",
  "dimension_data_not_exist": "The metric data does not exist.",
  "dimension_update_success": "Update successful.",
  "dimension_update_failed": "Update failed.",
  "dimension_name_required": "Name is required.",
  "dimension_back": "Exit",
  "dimension_test_run": "Trial run",
  "dimension_save": "Save",
  "file_import_back": "Exit",
  "file_import_name_label": "Name",
  "file_import_name_placeholder": "",
  "file_import_select_file": "Please select a file.",
  "file_import_success": "File imported successfully.",
  "file_import_file_label": "File",
  "file_import_download_template": "Download template",
  "file_import_download_template_tip": "Download the template to see the required file format.",
  "file_import_auto_evaluation_label": "Automatically evaluate the quality of imported data",
  "file_import_auto_evaluation_tip": "If enabled, a quality evaluation will be automatically performed on the imported data.",
  "file_import_evaluation_model_label": "Evaluation model",
  "file_import_evaluation_model_placeholder": "Select",
  "file_import_confirm": "OK",
  "manage_dimension": "Settings",
  "selected_count": "Selected",
  "dimension_config_tip": "Dimension configuration instructions",
  "custom": "Custom",
  "select_model_placeholder": "Select",
  "create_new_task_modal": "Create task",
  "task_name_input": "Name",
  "task_name_input_placeholder": "",
  "evaluation_app_select": "App",
  "evaluation_app_support_tip": "You can select a simple app or workflow but not a plugin.",
  "evaluation_app_select_placeholder": "Select",
  "evaluation_app_version_select": "App version",
  "evaluation_app_version_select_placeholder": "Select",
  "evaluation_dataset_select": "Dataset",
  "evaluation_dataset_select_placeholder": "Select",
  "create_import_dataset": "Create/Import",
  "evaluation_dimensions_label": "Metrics",
  "evaluation_dimensions_recommendation": "The evaluation involves knowledge base queries and AI chats. It is recommended to use {{num}} metrics for evaluation.",
  "builtin_dimension": "Predefined",
  "custom_dimension": "Custom",
  "config_params": "Settings",
  "score_aggregation_method": "Score aggregation",
  "evaluation_dimensions": "Metrics",
  "dimension": "Metric",
  "judgment_threshold": "Threshold",
  "judgment_threshold_tip": "Determines whether a metric's score meets expectations. If it is lower than the specified threshold, it will be marked as Below expectation. Valid range: 1-100. After the evaluation is complete, you can adjust it as needed.",
  "comprehensive_score_weight": "Weight",
  "comprehensive_score_weight_tip": "The ration of a metric's score to the total score. You can configure it as needed. After the evaluation is complete, you can adjust it as needed.",
  "comprehensive_score_weight_sum": "Total weight:",
  "intelligent_generation_dataset": "Auto generate dataset",
  "dataset_name_input": "Name",
  "dataset_name_input_placeholder": "Dataset name",
  "generation_basis": "Data source",
  "select_knowledge_base": "Select knowledge base",
  "data_amount": "Data size",
  "generation_model": "Model",
  "generation_model_placeholder": "Select",
  "high_quality": "High quality",
  "needs_improvement": "Pending optimization",
  "detail_evaluating": "Evaluating",
  "abnormal": "Evaluation error",
  "not_evaluated": "Not evaluated",
  "modify_result": "Modify",
  "restart_evaluation": "Evaluate again",
  "evaluation_error_message": "Error occurred during the evaluation. Please try again.",
  "high_quality_feedback": "The question is complete and clear, meeting the standard. The answer is accurate and practical.",
  "needs_improvement_feedback": "The question needs to be optimized. To obtain a more accurate answer, please provide more context to make the question more specific and clear.",
  "evaluation_service_error": "Evaluation encountered error.",
  "edit_data": "Edit data",
  "enter_question": "",
  "question_required": "Question is required.",
  "reference_answer": "Answer for reference",
  "enter_reference_answer": "",
  "reference_answer_required": "Answer for reference is required.",
  "quality_evaluation": "Auto quality evaluation",
  "cancel": "Cancel",
  "save": "Save",
  "save_and_next": "Save and proceed",
  "manually_calibrated": "Results modified.",
  "modify_evaluation_result_title": "Modify result",
  "evaluation_result_label": "Evaluation result",
  "modify_reason_label": "Reason",
  "modify_reason_input_placeholder": "Reason",
  "no_data": "No data available.",
  "search": "Search",
  "settings": "Settings",
  "add_data": "Add data",
  "ai_generate": "Auto generate",
  "manual_add": "Manually add",
  "no_answer": "No answers available.",
  "confirm_delete_data": "Are you sure you want to delete this entry?",
  "no_evaluation_result_click": "No evaluation results available. ",
  "start_evaluation_action": "start evaluation.",
  "evaluation_dataset": "Dataset",
  "evaluation_status": "Status",
  "manually_add_data_modal": "Add evaluation data",
  "question_input_label": "Question",
  "max_chars_3000_placeholder": "Up to 3,000 characters allowed",
  "reference_answer_input_label": "Expected answer",
  "auto_quality_eval_after_add": "Quality evaluation",
  "auto_quality_eval_add_tip": "If enabled, Q&A relevance and correctness are automatically evaluated, providing evaluation results and optimization support.",
  "quality_eval_model_label": "Evaluation model",
  "select_quality_eval_model_placeholder": "Select",
  "confirm": "OK",
  "confirm_quality_evaluation": "Are you sure you want to evaluate the quality of all data ({{total}})?",
  "model_change_notice": "The new model will apply to subsequent evaluation tasks.",
  "evaluation_model": "Model",
  "select_evaluation_model": "Select",
  "evaluation_abnormal": "Evaluation error",
  "error_message": "Error",
  "file_parse_error": "File parsing error",
  "delete_file": "Delete file",
  "reparse": "Reparse",
  "data_generation_error": " entries encountered error during generation.",
  "source_knowledge_base": "Based on knowledge base",
  "source_chunk": "Based on chunk",
  "operations": "Operation",
  "retry": "Retry",
  "retry_all": "Retry all",
  "error_info": "Error",
  "manual_add_data": "Add evaluation data",
  "max_3000_chars": "Up to 3,000 characters allowed",
  "please_enter_question": "",
  "question_max_3000_chars": "Cannot exceed 3,000 characters.",
  "please_enter_reference_answer": "",
  "reference_answer_max_3000_chars": "Cannot exceed 3,000 characters.",
  "auto_quality_evaluation": "Automatically evaluate quality of the added data.",
  "quality_evaluation_tip": "Automatically evaluate Q&A relevance and correctness, providing evaluation results and optimization support.",
  "quality_evaluation_model": "Evaluation model",
  "please_select_evaluation_model": "Select",
  "summary_pending": "Pending generation",
  "summary_generating": "Generating",
  "summary_done": "Completed",
  "summary_failed": "Failed",
  "method_mean": "Average",
  "method_median": "Median",
  "prompt_cannot_be_empty": "Prompt is required.",
  "please_select_model": "Please select a model.",
  "run_failed_please_retry": "Test run failed. Please try again.",
  "intelligent_generate_data": "Auto generate data",
  "evaluation_completed": "Evaluation completed.",
  "generation_error": "Generation error",
  "data_generating": "Generating data",
  "delete_dataset_error": "Error occurred while deleting the dataset.",
  "create_failed": "Creation failed.",
  "no_dimension_data": "No metric data available.",
  "please_select_dimension_first": "Please select a metric first.",
  "model_evaluation_tip": "The language model determines whether the actual and expected answers match.\nThe index model converts the actual and expected answers into vectors to further evaluate semantic similarity.",
  "create_new_dimension": "Add metric",
  "retry_success": "Retry successfully",
  "data_generation_error_count": "{{count}} pieces of data generated exception",
  "builtin_answer_correctness_name": "Answer Correctness",
  "builtin_answer_correctness_desc": "Evaluates the factual consistency between the generated answer and the reference answer, evaluating whether it is accurate and error-free.",
  "builtin_answer_similarity_name": "Answer Similarity",
  "builtin_answer_similarity_desc": "Evaluates the semantic alignment between the generated answer and the reference answer, determining whether they convey the same core information.",
  "builtin_answer_relevancy_name": "Answer Relevance",
  "builtin_answer_relevancy_desc": "Evaluates how well the generated answer aligns with the question, judging whether the response directly addresses the query.",
  "builtin_faithfulness_name": "Faithfulness",
  "builtin_faithfulness_desc": "Evaluates whether the generated answer remains faithful to the provided context, determining whether it contains fabricated or inaccurate content.",
  "builtin_context_recall_name": "Context Recall",
  "builtin_context_recall_desc": "Evaluates whether the retrieval system successfully retrieves all key information necessary for formulating the answer, assessing the completeness of retrieval.",
  "builtin_context_precision_name": "Context Precision",
  "builtin_context_precision_desc": "Evaluates whether high-value information is prioritized in the retrieved content, reflecting the quality of ranking and information density.",
  "join_evaluation_dataset": "Add to dataset",
  "not_join_evaluation_dataset": "Do not add to dataset",
  "create_new_dataset_btn_text": "Add to dataset",
  "please_select_evaluation_dataset": "Select",
  "join_knowledge_base": "Add to knowledge base",
  "all_data_with_count": "All data ({{num}}）",
  "question_data_with_count": "Problem data（{{num}}）",
  "error_data_with_count": "Error data（{{num}}）",
  "export_data": "Export",
  "retry_action": "Retry",
  "basic_info": "Basics",
  "application": "App",
  "version": "Version",
  "evaluation_dataset_name": "Datasets",
  "start_time": "Start time",
  "end_time": "End time",
  "executor_name": "Operator",
  "app_with_search_and_chat_recommendation": "The evaluation involves knowledge base search and AI chats. It is recommended to use 3 metrics for evaluation.",
  "app_with_chat_recommendation": "The evaluation involves AI chats. It is recommended to use 1 metric for evaluation.",
  "app_with_search_recommendation": "The evaluation involves knowledge base search. It is recommended to use 2 metrics for evaluation.",
  "no_dimensions_added": "No metrics available. ",
  "click_to_add": "Add",
  "meets_expectation": " meets expectation.",
  "below_expectation": " is below expectation.",
  "summary_generation_error": "Error occurred while generating a summary. Retry",
  "error_message_prefix": "Error: ",
  "summary_pending_generation": "Pending",
  "summary_generating_content": "Generating",
  "data_with_count": "Cases ({{data}})",
  "search_placeholder": "Search",
  "detail_title": "Details",
  "modify_dataset_simultaneously": "Sync changes to dataset",
  "retry_button": "Retry",
  "edit_action": "Edit",
  "delete_action": "Delete",
  "confirm_delete_data_in_task": "Are you sure you want to delete the entry?",
  "view_full_response": "Complete response",
  "abnormal_status": "Error",
  "question_field": "Question",
  "reference_answer_field": "Expected Answer",
  "actual_answer_field": "Actual answer",
  "no_answer_available": "No answers available.",
  "comprehensive_score_title": "Rating",
  "dimension_score_title": "Metric score",
  "error_data_calculation_notice": "{{count}} cases encountered error during execution. Only successful cases are used to calculate the score.",
  "regenerate_summary_content": "Generate summary again",
  "processing_status": "Processing...",
  "view_results_after_completion": "Please wait until the evaluation is complete.",
  "all_data_execution_error": "All entries encountered error during execution.",
  "check_error_details": "Please view the cause.",
  "click_to_retry": "Retry",
  "comprehensive_score_weight_description": "The ratio of a metric's score to the total score. You can configure it as needed.",
  "no_data_available": "No data available.",
  "question_column": "Question",
  "comprehensive_score_column": "Rating",
  "error_info_column": "Error",
  "request_failed": "Request failed.",
  "retry_request_submitted": "Request sent successfully.",
  "retry_failed": "Retry failed.",
  "save_success": "Saved successfully.",
  "save_failed": "Operation failed.",
  "summary_generation_request_submitted": "Request sent successfully.",
  "generate_summary_failed": "Failed to generate a summary.",
  "export_failed": "Export failed.",
  "load_failed": "Loading failed.",
  "export_success": "Export successful.",
  "no_dimension_data_cannot_generate_summary": "Unable to generate a summary because no metric data is available.",
  "Task_name": "Task name",
  "click_to_download_template": "Download CSV template",
  "evaluation_created": "Evaluation task created successfully."
}