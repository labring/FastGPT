import type { HelperBotDispatchParamsType, HelperBotDispatchResponseType } from '../type';
import { helperChats2GPTMessages } from '@fastgpt/global/core/chat/helperBot/adaptor';
import { getPrompt } from './prompt';
import { createLLMResponse } from '../../../../ai/llm/request';
import { getLLMModel } from '../../../../ai/model';
import { SseResponseEventEnum } from '@fastgpt/global/core/workflow/runtime/constants';
import { textAdaptGptResponse } from '@fastgpt/global/core/workflow/runtime/utils';
import { generateResourceList } from './utils';
import { TopAgentFormDataSchema } from './type';
import { addLog } from '../../../../../common/system/log';
import { formatAIResponse } from '../utils';

export const dispatchTopAgent = async (
  props: HelperBotDispatchParamsType
): Promise<HelperBotDispatchResponseType> => {
  const { query, files, metadata, histories, workflowResponseWrite, user } = props;

  const modelData = getLLMModel();
  if (!modelData) {
    return Promise.reject('Can not get model data');
  }

  const usage = {
    model: modelData.model,
    inputTokens: 0,
    outputTokens: 0
  };

  const resourceList = await generateResourceList({
    teamId: user.teamId,
    isRoot: user.isRoot
  });
  const systemPrompt = getPrompt({ resourceList });

  const historyMessages = helperChats2GPTMessages({
    messages: histories,
    reserveTool: false
  });
  const conversationMessages = [
    { role: 'system' as const, content: systemPrompt },
    ...historyMessages,
    { role: 'user' as const, content: query }
  ];

  // console.log('ðŸ“ TopAgent é˜¶æ®µ 1: ä¿¡æ¯æ”¶é›†');
  // console.log('conversationMessages:', conversationMessages);

  const llmResponse = await createLLMResponse({
    body: {
      messages: conversationMessages,
      model: modelData,
      stream: true
    },
    onStreaming: ({ text }) => {
      workflowResponseWrite?.({
        event: SseResponseEventEnum.answer,
        data: textAdaptGptResponse({ text })
      });
    },
    onReasoning: ({ text }) => {
      workflowResponseWrite?.({
        event: SseResponseEventEnum.answer,
        data: textAdaptGptResponse({ reasoning_content: text })
      });
    }
  });
  usage.inputTokens = llmResponse.usage.inputTokens;
  usage.outputTokens = llmResponse.usage.outputTokens;

  /* 
    3 ç§è¿”å›žæƒ…å†µ
      1. ã€Œä¿¡æ¯æ”¶é›†å·²å®Œæˆã€
      2. JSON å­—ç¬¦ä¸²ï¼š{ reasoning?: string; question?: string }
      3. é…ç½®è¡¨å•
  */
  const firstPhaseAnswer = llmResponse.answerText;
  const firstPhaseReasoning = llmResponse.reasoningText;

  // å°è¯•è§£æžä¿¡æ¯æ”¶é›†é˜¶æ®µçš„ JSON å“åº”
  let parsedResponse: { reasoning?: string; question?: string } | null = null;
  try {
    parsedResponse = JSON.parse(firstPhaseAnswer);
  } catch (e) {
    // å¦‚æžœè§£æžå¤±è´¥,è¯´æ˜Žä¸æ˜¯ JSON æ ¼å¼,å¯èƒ½æ˜¯æ™®é€šæ–‡æœ¬
    parsedResponse = null;
  }

  if (firstPhaseAnswer.includes('ã€Œä¿¡æ¯æ”¶é›†å·²å®Œæˆã€')) {
    addLog.debug('ðŸ”„ TopAgent: æ£€æµ‹åˆ°ä¿¡æ¯æ”¶é›†å®Œæˆä¿¡å·ï¼Œåˆ‡æ¢åˆ°è®¡åˆ’ç”Ÿæˆé˜¶æ®µ');

    const newMessages = [
      ...conversationMessages,
      { role: 'assistant' as const, content: firstPhaseAnswer },
      { role: 'user' as const, content: 'è¯·ä½ ç›´æŽ¥ç”Ÿæˆè§„åˆ’æ–¹æ¡ˆ' }
    ];

    const planResponse = await createLLMResponse({
      body: {
        messages: newMessages,
        model: modelData,
        stream: true
      },
      onStreaming: ({ text }) => {
        workflowResponseWrite?.({
          event: SseResponseEventEnum.answer,
          data: textAdaptGptResponse({ text })
        });
      },
      onReasoning: ({ text }) => {
        workflowResponseWrite?.({
          event: SseResponseEventEnum.answer,
          data: textAdaptGptResponse({ reasoning_content: text })
        });
      }
    });
    usage.inputTokens = planResponse.usage.inputTokens;
    usage.outputTokens = planResponse.usage.outputTokens;

    try {
      const planJson = JSON.parse(planResponse.answerText);

      const formData = TopAgentFormDataSchema.parse({
        role: planJson.task_analysis?.role,
        taskObject: planJson.task_analysis?.goal,
        tools: planJson.resources?.tools?.map((tool: any) => tool.id),
        fileUploadEnabled: planJson.resources?.system_features?.file_upload?.enabled || false
      });

      // Send formData if exists
      if (formData) {
        workflowResponseWrite?.({
          event: SseResponseEventEnum.formData,
          data: formData
        });
      }
    } catch (e) {
      addLog.warn(`[Top agent] parse answer faield`, { text: planResponse.answerText });
    }

    return {
      aiResponse: formatAIResponse({
        text: planResponse.answerText,
        reasoning: planResponse.reasoningText
      }),
      usage
    };
  }

  const displayText = parsedResponse?.question || firstPhaseAnswer;
  return {
    aiResponse: formatAIResponse({ text: displayText, reasoning: firstPhaseReasoning }),
    usage
  };
};
